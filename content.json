{"meta":{"title":"AIBluefisher","subtitle":null,"description":null,"author":"AIBluefisher","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"LIFT","slug":"LIFT","date":"2018-04-26T09:39:18.000Z","updated":"2018-04-27T04:55:18.988Z","comments":true,"path":"2018/04/26/LIFT/","link":"","permalink":"http://yoursite.com/2018/04/26/LIFT/","excerpt":"LIFT[1]是一种使用CNN来提取特征点(feature points / interests)的方法, 发表在ECCV 2016. 众所周知, 尽管在2004年就已经发表出来, 但是SIFT(Scale Invariant Feature Transform)在三维重建中一直至今都无可替代. 随着学习的火热, 研究人员开始将目光更多转向于寻求一种基于学习的方法来提取特征点, 从而代替人工设计的特征点.","text":"LIFT[1]是一种使用CNN来提取特征点(feature points / interests)的方法, 发表在ECCV 2016. 众所周知, 尽管在2004年就已经发表出来, 但是SIFT(Scale Invariant Feature Transform)在三维重建中一直至今都无可替代. 随着学习的火热, 研究人员开始将目光更多转向于寻求一种基于学习的方法来提取特征点, 从而代替人工设计的特征点. 1.MotivationSIFT提取特征点包括三个步骤: 关键点检测器检测候选特征点、方向赋值、计算128维描述子向量. 在LIFT之前的工作中, 有很多使用CNN来学习这三个步骤的算法, 然而LIFT是第一个提出将这三个网络整合到一起并提出成功的训练方法. LIFT的作者发现, 从零开始训练整个网络是不可能的, 因为这三个神经网络的目的不同. 因此, 他们提出分别训练这三个网络来解决这个问题: 首先训练描述子(Descriptor), 然后是方向估计子(Orientation Estimator), 最后是特征点检测器(detector). 2.训练数据LIFT的训练数据来源于使用Structure from Motion(SfM)算法得到的特征点, 而SfM算法的输入为在不同视点(viewpoints)和光照条件下获取到的图像. 实际上, LIFT使用的训练数据并非一个个像素点, 而是一些图像块(image patches). 为了使得优化过程更易于处理, 这些图像块在不同的尺度空间中提取得到(这句话没太理解, 先留作疑问. 原文是: “We formulate this training problem on image patches extracted at different scales to make the optimization tractable”). 3.方法3.1 训练数据的构建使用Piccadilly和Roman Forum和Visual SFM来重建三维场景. 其中Piccadilly数据集包含3384张图片, 最后的重建结果包含59k个特征点(平均每张图片6.5个特征点); Roman-Forum数据集包含1658张图片, 最后的重建结果包含51k个特征点(平均每张图片包含5.2个特征点). 作者将这些数据分成训练集和验证集. 为了构建一个正的训练样本, 他们只使用了在SfM重建之后保留下的特征点(不是使用最初SIFT提取出来的特征点). 运行时的pipeline 3.2 三个网络这部分内容还没看太懂, 之后再更~ 4.实验结果及对比LIFT的作者使用了三个数据集进行对比实验: Strecha数据集. 包含从不同视角拍摄的两个场景的19张图片.DTU数据集. 包含在不同视点和光照条件下拍摄的60张图片. 这个数据集用于检测LIFT在视点变化下的性能.Webcam数据集. 包含同一视点拍摄的强光照变化下的6个场景的710张图片. 这个数据集用于检测LIFT在自然光照变化下的性能..对于Strcha和DTU数据集, LIFT的作者使用所提供的ground truth来进行匹配. 同时, 他们使用三个度量标准来评估性能: 可重复性(repeatability, Rep): 这个标准用于检测feature detector的性能.最近邻mAP(Nearest Neighbor mean Average Precision, NN mAP)匹配得分(Matching Score, M. Score): 匹配点的ground truth可以被恢复出来的比例. 这个标准用于评估整个算法的性能. 4.1 匹配结果对比下图展示了分别使用SIFT(左边)和LIFT(右边)在Piccadilly数据集训练之后, 在500个特征点的图像上的匹配结果. 实验结果表明LIFT能得到更多正确的匹配. 4.2 整个LIFT网络评估对于整个网络的性能评估, 论文中和多种传统方法以及基于学习的方法进行了对比. 下图给出了在三个数据集中各个方法得到的average matching score. 下表给出了各个方法得到的average matching score的确切数字. 其中LIFT-pic是使用Piccadilly数据集训练, LIFT-rf使用Roman-Forum训练. 结果显示在这三个数据集上, LIFT都超过了其他方法. 其中一个有趣的结果是: 在DTU数据集上, SIFT仍然是除LIFT之外的性能最好的方法(包括使用其他基于学习方法的特征点). 最后, 为了了解LIFT中, detector, orientation estimator, descriptor对于整个LIFT的影响, 作者还将LIFT中每个部分得到的结果和SIFT交换, 并比较了它们的性能, 结果如下表. 结果表明, LIFT中每一个部分得到的结果都超过了SIFT的每一个部分, 并在LIFT的整个过程中都起到了重要作用. 5.缺点/不足尽管LIFT已经很优秀了, 但是它还存在一些明显的不足: LIFT使用Spatial Transformer和softargmax将之前工作中三个独立的网络整合到一个统一的网络中, 并且能够使用反向传播算法进行端到端的训练(感觉地位和faster-RCNN相似), 但是这也使得整个网络从零开始学习尽管在理论上可行, 实际上却是不行的. 因此, 还需要一个更有效的策略来对LIFT进行训练. 除此之外, 由于只使用Internet数据集进行训练, LIFT还表现出对数据的过拟合[2], 因此下一个目标应该是使用更多数据集进行训练. Future work寻找一种更有效的训练策略来对整个网络进行训练使用hard negative mining策略来使用整张图片而不只是图片块对LIFT进行训练 6. 补充材料看视频应该更能直观地感受LIFT的效果: https://www.youtube.com/watch?v=hhxAttChmCo 以及, 关于LIFT的所有材料都能在这个网站上找到: https://cvlab.epfl.ch/research/detect/lift LIFT也在Github上公布了模型及其tensorflow实现: 模型 - https://github.com/cvlab-epfl/LIFT tensorflow实现 - https://github.com/cvlab-epfl/tf-lift 参考文献[1] Yi K M, Trulls E, Lepetit V, et al. LIFT: Learned Invariant Feature Transform[C]// European Conference on Computer Vision. Springer, Cham, 2016:467-483. [2] Schonberger J L, Hardmeier H, Sattler T, et al. Comparative Evaluation of Hand-Crafted and Learned Local Features[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:6959-6968.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]}]}