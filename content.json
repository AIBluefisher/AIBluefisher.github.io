{"meta":{"title":"AIBluefisher","subtitle":null,"description":null,"author":"AIBluefisher","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Introduction to Structure from Motion","slug":"sfm-md","date":"2019-04-04T01:19:14.000Z","updated":"2019-04-04T01:58:12.581Z","comments":true,"path":"2019/04/04/sfm-md/","link":"","permalink":"http://yoursite.com/2019/04/04/sfm-md/","excerpt":"1. Incremental Structure from Motion [x] [3DV 2013] Towards Linear-time Incremental Structure from Motion 第一次提出 Retriangulate 的概念来尝试解决incremental SfM中的drift.由于初始的相机姿态和经过BA之后的姿态可能都不够准，一些正确的匹配在triangulate的时候失败了。而这些正确匹配的累计丢失是造成drift的一个主因。 为了解决这个问题，VSFM提出对失败的匹配进行 re-triangulate(RT)。RT的时候，需要选择相机对。","text":"1. Incremental Structure from Motion [x] [3DV 2013] Towards Linear-time Incremental Structure from Motion 第一次提出 Retriangulate 的概念来尝试解决incremental SfM中的drift.由于初始的相机姿态和经过BA之后的姿态可能都不够准，一些正确的匹配在triangulate的时候失败了。而这些正确匹配的累计丢失是造成drift的一个主因。 为了解决这个问题，VSFM提出对失败的匹配进行 re-triangulate(RT)。RT的时候，需要选择相机对。 怎么识别不好的相机姿态: 两个相机之间的公共三维点和匹配点数目的比例太低。这样的相机对被称为under-recontructed。为了使得相机姿态更稳定，需要对这些under-reconstructed的相机进行re-triangulate。为了得到更多的点，冲投影误差的阈值加大。从上到下为Without RT、With RT : [x] [CVPR 2016] Structure-from-Motion Revisited [x] [3DV 2017] Batched Incremental Structure-from-Motion 主要贡献点在于batched camera registration和batched tracks selection。 (1) 不同于COLMAP，在对相机进行注册的时候不再考虑使用next view selection的策略，而是把有足够多2D-3D点对的相机批量式(batched)进行注册。这样做的原因是: incremental SfM的误差是累积的，更早注册的相机要比后注册的相机姿态更为准确。 (2) batched tracks selection的意思是，在每个相机注册的步骤，不再考虑所有的track，而是仅把track的一部分进行选择并且优化。对于大规模重建了来说，track通常是冗余的并且对于BA来说既耗内存也耗时间。为了保证下一个相机注册的成功率，track的选择有三个原则: 包含已标定的相机，来优化它们的姿态 包含将要在下一个步骤中进行标定的相机 更长的track应该优先选择 在进行增量重建的过程中，不使用匹配点而使用track。原因: track的长度通常比较长，使用track能够在相机注册的时候得到更多的候选。 track构建的时候，很多错误的匹配已经被筛选出来了。 track是feature match的集合，相比feature match来说更可信。 流程如下： 1. Batched Seed Selection初始对的选取原则：(1) 能看到尽可能多的相机(2) 宽基线 细节在构造Epipolar Graph的时候，先用初始的EG中的相对相机姿态通过rotation averaging(RA)计算出global的相机姿态。然后通过计算acos(||R_{ij} - R_jR_i^T||_F) 来将误差大的边删除，然后相对旋转通过 R_{ij}=R_jR_i^T重新计算得到。然后，对于EG中的每条边，通过 $\\rho{ij} = min(n_i, n_j)$ 来表示与至相邻的相机密度, 其中$n_i$是相机$i$在EG中相邻相机的个数。最后，得到以 $\\rho{ij}$ 的大小为优先级的候选seeds. 2. Batched Camera Registration所有能够看到大于12个三维点的相机都考虑为候选标定相机，然后RANSAC + P3P计算初始相机姿态，最后加bundle adjustment优化。这时固定三维点和内参，只对外参进行优化。 3. Tracks Triangulation and Selection使用track作为输入，每次选两个相机，使用RANSAC + Triangulation来计算三维点。一个较为稳定的三维点应满足: 投影回图像平面的两条射线的家教应该大于 $2^{\\circ}$ 满足cheirality约束(在相机前方，具有正的深度值) 为了提高效率，不使用所有的track，而是找track的子集，这些track需要对每个相机至少包含 $K$ 次 4. Batched Bundle Adjustment5. Iterative Re-triangulation and Re-selection结果 2. Global Structure from Motion3. Hierarchical Structure from Motion4. Hybrid Structure from Motion [x] [ACCV 2014] Divide and Conquer: Efficient Large-Scale Structure from Motion Using Graph Partitioning [x] [arXiv 2017] Parallel Structure from Motion from Local Increment to Global Averaging [x] [CVPR 2018] Very Large-scale Global SfM by Distributed Motion Averaging","categories":[],"tags":[{"name":"structure from motion","slug":"structure-from-motion","permalink":"http://yoursite.com/tags/structure-from-motion/"}]},{"title":"LIFT","slug":"LIFT","date":"2018-04-26T09:39:18.000Z","updated":"2018-04-27T04:55:18.988Z","comments":true,"path":"2018/04/26/LIFT/","link":"","permalink":"http://yoursite.com/2018/04/26/LIFT/","excerpt":"LIFT[1]是一种使用CNN来提取特征点(feature points / interests)的方法, 发表在ECCV 2016. 众所周知, 尽管在2004年就已经发表出来, 但是SIFT(Scale Invariant Feature Transform)在三维重建中一直至今都无可替代. 随着学习的火热, 研究人员开始将目光更多转向于寻求一种基于学习的方法来提取特征点, 从而代替人工设计的特征点.","text":"LIFT[1]是一种使用CNN来提取特征点(feature points / interests)的方法, 发表在ECCV 2016. 众所周知, 尽管在2004年就已经发表出来, 但是SIFT(Scale Invariant Feature Transform)在三维重建中一直至今都无可替代. 随着学习的火热, 研究人员开始将目光更多转向于寻求一种基于学习的方法来提取特征点, 从而代替人工设计的特征点. 1.MotivationSIFT提取特征点包括三个步骤: 关键点检测器检测候选特征点、方向赋值、计算128维描述子向量. 在LIFT之前的工作中, 有很多使用CNN来学习这三个步骤的算法, 然而LIFT是第一个提出将这三个网络整合到一起并提出成功的训练方法. LIFT的作者发现, 从零开始训练整个网络是不可能的, 因为这三个神经网络的目的不同. 因此, 他们提出分别训练这三个网络来解决这个问题: 首先训练描述子(Descriptor), 然后是方向估计子(Orientation Estimator), 最后是特征点检测器(detector). 2.训练数据LIFT的训练数据来源于使用Structure from Motion(SfM)算法得到的特征点, 而SfM算法的输入为在不同视点(viewpoints)和光照条件下获取到的图像. 实际上, LIFT使用的训练数据并非一个个像素点, 而是一些图像块(image patches). 为了使得优化过程更易于处理, 这些图像块在不同的尺度空间中提取得到(这句话没太理解, 先留作疑问. 原文是: “We formulate this training problem on image patches extracted at different scales to make the optimization tractable”). 3.方法3.1 训练数据的构建使用Piccadilly和Roman Forum和Visual SFM来重建三维场景. 其中Piccadilly数据集包含3384张图片, 最后的重建结果包含59k个特征点(平均每张图片6.5个特征点); Roman-Forum数据集包含1658张图片, 最后的重建结果包含51k个特征点(平均每张图片包含5.2个特征点). 作者将这些数据分成训练集和验证集. 为了构建一个正的训练样本, 他们只使用了在SfM重建之后保留下的特征点(不是使用最初SIFT提取出来的特征点). 运行时的pipeline 3.2 三个网络这部分内容还没看太懂, 之后再更~ 4.实验结果及对比LIFT的作者使用了三个数据集进行对比实验: Strecha数据集. 包含从不同视角拍摄的两个场景的19张图片.DTU数据集. 包含在不同视点和光照条件下拍摄的60张图片. 这个数据集用于检测LIFT在视点变化下的性能.Webcam数据集. 包含同一视点拍摄的强光照变化下的6个场景的710张图片. 这个数据集用于检测LIFT在自然光照变化下的性能..对于Strcha和DTU数据集, LIFT的作者使用所提供的ground truth来进行匹配. 同时, 他们使用三个度量标准来评估性能: 可重复性(repeatability, Rep): 这个标准用于检测feature detector的性能.最近邻mAP(Nearest Neighbor mean Average Precision, NN mAP)匹配得分(Matching Score, M. Score): 匹配点的ground truth可以被恢复出来的比例. 这个标准用于评估整个算法的性能. 4.1 匹配结果对比下图展示了分别使用SIFT(左边)和LIFT(右边)在Piccadilly数据集训练之后, 在500个特征点的图像上的匹配结果. 实验结果表明LIFT能得到更多正确的匹配. 4.2 整个LIFT网络评估对于整个网络的性能评估, 论文中和多种传统方法以及基于学习的方法进行了对比. 下图给出了在三个数据集中各个方法得到的average matching score. 下表给出了各个方法得到的average matching score的确切数字. 其中LIFT-pic是使用Piccadilly数据集训练, LIFT-rf使用Roman-Forum训练. 结果显示在这三个数据集上, LIFT都超过了其他方法. 其中一个有趣的结果是: 在DTU数据集上, SIFT仍然是除LIFT之外的性能最好的方法(包括使用其他基于学习方法的特征点). 最后, 为了了解LIFT中, detector, orientation estimator, descriptor对于整个LIFT的影响, 作者还将LIFT中每个部分得到的结果和SIFT交换, 并比较了它们的性能, 结果如下表. 结果表明, LIFT中每一个部分得到的结果都超过了SIFT的每一个部分, 并在LIFT的整个过程中都起到了重要作用. 5.缺点/不足尽管LIFT已经很优秀了, 但是它还存在一些明显的不足: LIFT使用Spatial Transformer和softargmax将之前工作中三个独立的网络整合到一个统一的网络中, 并且能够使用反向传播算法进行端到端的训练(感觉地位和faster-RCNN相似), 但是这也使得整个网络从零开始学习尽管在理论上可行, 实际上却是不行的. 因此, 还需要一个更有效的策略来对LIFT进行训练. 除此之外, 由于只使用Internet数据集进行训练, LIFT还表现出对数据的过拟合[2], 因此下一个目标应该是使用更多数据集进行训练. Future work寻找一种更有效的训练策略来对整个网络进行训练使用hard negative mining策略来使用整张图片而不只是图片块对LIFT进行训练 6. 补充材料看视频应该更能直观地感受LIFT的效果: https://www.youtube.com/watch?v=hhxAttChmCo 以及, 关于LIFT的所有材料都能在这个网站上找到: https://cvlab.epfl.ch/research/detect/lift LIFT也在Github上公布了模型及其tensorflow实现: 模型 - https://github.com/cvlab-epfl/LIFT tensorflow实现 - https://github.com/cvlab-epfl/tf-lift 参考文献[1] Yi K M, Trulls E, Lepetit V, et al. LIFT: Learned Invariant Feature Transform[C]// European Conference on Computer Vision. Springer, Cham, 2016:467-483. [2] Schonberger J L, Hardmeier H, Sattler T, et al. Comparative Evaluation of Hand-Crafted and Learned Local Features[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:6959-6968.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]}]}