<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | AIBluefisher</title>
    <link>https://AIBluefisher.github.io/post/</link>
      <atom:link href="https://AIBluefisher.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2020</copyright><lastBuildDate>Sun, 28 Jul 2019 11:02:09 +0800</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://AIBluefisher.github.io/post/</link>
    </image>
    
    <item>
      <title>Robust Optimization in 3D Vision</title>
      <link>https://AIBluefisher.github.io/post/optimization/</link>
      <pubDate>Sun, 28 Jul 2019 11:02:09 +0800</pubDate>
      <guid>https://AIBluefisher.github.io/post/optimization/</guid>
      <description>&lt;h2 id=&#34;1-凸优化和非凸优化&#34;&gt;1. 凸优化和非凸优化&lt;/h2&gt;
&lt;p&gt;凸优化问题的一般形式&lt;/p&gt;
&lt;p&gt;$$
\min\quad f(x) \qquad\qquad\qquad\qquad \&lt;br&gt;
s.t.\quad h_i(x) = 0, \forall i = 1,\cdots, n\&lt;br&gt;
\qquad\quad g_j(x) &amp;lt; 0, \forall j = 1, \cdots, m
\tag{1}
$$
其中 $f(x), g_j(x)$ 都是凸函数，$h_i(x)$ 为仿射函数(既是凸函数又是凹函数)。&lt;/p&gt;
&lt;h3 id=&#34;11-凸函数&#34;&gt;1.1 凸函数&lt;/h3&gt;
&lt;p&gt;凸函数的定义:
对于定义在凸集上的函数$f$，如果对任意 $0 \leq \theta \leq 1$，都有&lt;/p&gt;
&lt;p&gt;$$
f(\theta x + (1 - \theta) y) \leq \theta f(x) + (1 - \theta) f(y) \tag{2}
$$&lt;/p&gt;
&lt;p&gt;一个简单的例子: 对于下图，函数任意两点之间的弦都在函数的上方。&lt;/p&gt;
&lt;div align=center&gt; 
    &lt;img src=&#34;https://AIBluefisher.github.io/img/optimization/convex_function.png&#34; /&gt;
&lt;/div&gt;
&lt;!-- 以及更高维空间中的凸函数

&lt;div align=center&gt;
    &lt;img src=&#34;https://AIBluefisher.github.io/img/optimization/3d_convex_function.jpg&#34;/&gt;
&lt;/div&gt; --&gt;
&lt;p&gt;但是实际中，要验证一个函数是否是凸函数，这个定义很不好用。因此，存在一些其他方法判断是否是凸函数。&lt;/p&gt;
&lt;h4 id=&#34;1-导数和二阶信息判断&#34;&gt;(1) 导数和二阶信息判断&lt;/h4&gt;
&lt;p&gt;对于一个光滑的函数(几乎处处可导)，可通过以下导数信息判断:&lt;/p&gt;
&lt;p&gt;(一阶充要条件) $f(y) \geq f(x) + \nabla f(x)^T (y - x)$
(二阶充要条件) $\nabla^2 f(x) \geq 0$&lt;/p&gt;
&lt;h4 id=&#34;2-利用凸函数的结构叠加性质&#34;&gt;(2) 利用凸函数的结构叠加性质&lt;/h4&gt;
&lt;p&gt;另一类有用的方法就是可以利用凸函数的一些结构叠加性质来判断这些函数是否是凸函数复合而成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;非负加权求和: $f(x) = \sum_i \alpha_if_i(x), \forall \alpha_i \geq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;仿射变换: $f(x) = f_i(Ax + b)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;取最大值: $f(x) = max{f_1(x), \cdots, f_m(x)}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标量复合, 向量复合, 最小化 &amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;12-非凸函数&#34;&gt;1.2 非凸函数&lt;/h3&gt;
&lt;p&gt;凸函数(凹函数，拟凸函数)的一个对立就是非凸函数。凸函数的一个好处就是，对于优化得到的局部最优点，一定是一个全局最优点，因为凸函数只存在一个全局最优。而对于非凸函数，则存在非常多的局部最优点，还有非常多的鞍点。如下图所示:&lt;/p&gt;
&lt;div align=center&gt;
    &lt;img src=&#34;https://AIBluefisher.github.io/img/optimization/non_convex_function.jpg&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;对于非凸优化问题，当优化算法收敛的时候，不利用其他信息，很难判断是否收敛到全局最优。因而非凸优化问题的求解要想尽可能的逼近全局最优，需要有较为可靠的初始值。很不幸的是，我们面临的大多数问题都是非凸优化问题。&lt;/p&gt;
&lt;h2 id=&#34;2-l_1-范数-vs-l_2-范数&#34;&gt;2. $l_1$-范数 VS $l_2$-范数&lt;/h2&gt;
&lt;p&gt;对于优化问题，我们通常想要最小化估计值 $f(x)$ 和目标值 $y$ 之间的残差(residual)&lt;/p&gt;
&lt;p&gt;$$
\min d(y, f(x))^p　\tag{3}
$$&lt;/p&gt;
&lt;p&gt;其中，$d(,)^p$ 表示误差度量的范数。对于向量来说，常见的有 $l_1$-范数、$l_2$-范数。
如果是 $l_1$-范数，残差是目标值和估计值之差的绝对值之和:&lt;/p&gt;
&lt;p&gt;$$
S = \sum_{i=1}^n |y_i - f(x_i)| \tag{4}
$$&lt;/p&gt;
&lt;p&gt;如果是 $l_2$-范数，残差是目标值和估计值之差的平方和:&lt;/p&gt;
&lt;p&gt;$$
S = \sum_{i=1}^n (y_i - f(x_i))^2 \tag{5}
$$&lt;/p&gt;
&lt;p&gt;那么 使用　$l_1$-范数做优化和 $l_2$-范数来做优化有什么区别呢？
我们知道，真实的观测数据通常存在外点 (outliers)。outlier的存在会导致优化时的最优点偏离真实的最优点。在外点存在的情况下，$l_1$-范数会比$l_2$-范数鲁棒。虽然这在理论上并没有严格的证明，不过从直观上来讲，由于 $l_2$-范数对误差进行了平方，因此相比 $l_1$-范数来说，外点会贡献更多的误差(如果 $e &amp;gt; 1$, $e^2 &amp;gt; e$)。而优化算法又需要最小化误差，因此会导致优化会更多地朝着外点的方向去调整。&lt;/p&gt;
&lt;p&gt;这两种范数度量之间的比较:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$l_1$-范数比 $l_2$-范数更鲁棒&lt;/li&gt;
&lt;li&gt;$l_2$-范数比 $l_1$-范数更稳定&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-最小二乘法&#34;&gt;3. 最小二乘法&lt;/h2&gt;
&lt;h3 id=&#34;31-最小二乘问题&#34;&gt;3.1 最小二乘问题&lt;/h3&gt;
&lt;p&gt;最小二乘问题的定义是&lt;/p&gt;
&lt;p&gt;$$
\min f(x) = \frac{1}{2} \sum_{i=1}^m r_i^2(x) = \frac{1}{2}r(x)^Tr(x), x \in \mathbb{R}^n, m \geq n \tag{6}
$$&lt;/p&gt;
&lt;p&gt;这里 $r(x) = [r_1(x), r_2(x), \cdots, r_m(x)]^T$ 称作剩余函数(残差）。若 $r_i(x)$ 均为线性函数，则问题为&lt;code&gt;线性最小二乘问题&lt;/code&gt;；若至少有一个 $r_i(x)$ 为非线性函数，则问题为&lt;code&gt;非线性最小二乘问题&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;下面我们来看最小二乘问题的目标函数 $f(x)$ 的一、二阶导数的形式。设 $J(x)$ 是 $r(x)$ 的 Jacobi 矩阵:&lt;/p&gt;
&lt;p&gt;$$
J(x) = [\nabla r_1(x), \cdots, \nabla r_m(x)]^T \in \mathbb{R}^{m \times n}
$$&lt;/p&gt;
&lt;p&gt;则 $f(x)$ 的梯度为&lt;/p&gt;
&lt;p&gt;$$
g(x) = \sum_{i=1}^m r_i(x) \nabla r_i(x) = J(x)^T r(x) \tag{7}
$$&lt;/p&gt;
&lt;p&gt;$f(x)$ 的 Hessian 矩阵为&lt;/p&gt;
&lt;p&gt;$$
G(x) = \sum_{i=1}^m \nabla r_i(x) \nabla r_i(x)^T + \sum_{i=1}^m r_i(x)\nabla^2 r_i(x) = J(x)^T J(x) + S(x) \tag{8}
$$&lt;/p&gt;
&lt;p&gt;其中，$S(x) = \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)$。&lt;/p&gt;
&lt;h3 id=&#34;32-最小二乘的来源&#34;&gt;3.2 最小二乘的来源&lt;/h3&gt;
&lt;p&gt;最小二乘问题大量产生于数据拟合问题: 给定一组实验数据 $(x_i, y_i)$ 和函数模型 $f(x)$，要确定 $x$，使得 $f(x)$ 在残差的平方和意义下尽可能地拟合给定的数据。即&lt;/p&gt;
&lt;p&gt;$$
\arg\min_x \sum_{i=1}^n ||y_i - f(x_i)||_2^2 \tag{9}
$$&lt;/p&gt;
&lt;p&gt;最小二乘问题和极大似然估计之间存在联系。我们通常假定误差遵循高斯(正态)分布。具体来讲，我们假定目标值和观测值之间都具有零均值和标准差为 $\sigma$ 的高斯噪声。若真值为 $y_i$，估计值为 $f(x_i)$，那么每个估计值的概率密度函数是&lt;/p&gt;
&lt;p&gt;$$
Pr(x_i) = \frac{1}{2\pi \sigma^2} e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}} \tag{10}
$$&lt;/p&gt;
&lt;p&gt;假设误差独立同分布，那么联合概率密度函数为&lt;/p&gt;
&lt;p&gt;$$
Pr(x) = \prod_i Pr(x_i) = \prod_i \frac{1}{2\pi \sigma^2} e^{-\frac{(y_i - f(x_i))^2}{2\sigma^2}} \tag{11}
$$&lt;/p&gt;
&lt;p&gt;对应的对数似然函数为:&lt;/p&gt;
&lt;p&gt;$$
log Pr(x) = -\frac{1}{2\pi \sigma^2} \sum_i (y_i - f(x_i))^2 + c \tag{12}
$$&lt;/p&gt;
&lt;p&gt;最大似然估计最大化这个对数似然函数，也即最小化&lt;/p&gt;
&lt;p&gt;$$
\sum_i (y_i - f(x_i))^2 \Leftrightarrow \sum_i ||y_i - f(x_i)||_2^2 \tag{13}
$$&lt;/p&gt;
&lt;h3 id=&#34;33-鲁棒最小二乘和迭代重加权irls-问题&#34;&gt;3.3 鲁棒最小二乘和迭代重加权(IRLS) 问题&lt;/h3&gt;
&lt;p&gt;常规的最小二乘对其中噪声符合高斯分布的观测值来说是一个合适的选择，然而在存在外点的时，需要更鲁棒的最小二乘。这种情况下，最好用M-估计(M-estimator)，它对残差施加一个鲁棒惩罚函数 $\rho (r) $ (也称为 &lt;code&gt;loss function&lt;/code&gt;)
$$
E_{RLS(\triangle \boldsymbol{x})} = \sum_{i} \rho(||\boldsymbol{r}_i||) \tag{14}
$$
来代替它们的平方&lt;/p&gt;
&lt;p&gt;$$
E_{LS(\triangle \boldsymbol{x})} = \sum_{i} ||\boldsymbol{r}_i||_2^2 \tag{15}
$$&lt;/p&gt;
&lt;p&gt;其中 $\boldsymbol{r}_i = y_i - f(x_i)$ 。公式(14)对$\boldsymbol{x}$求偏导，则有&lt;/p&gt;
&lt;p&gt;$$
\sum_{i} \frac{\partial \rho (||\boldsymbol{r}_i||)}{\partial ||\boldsymbol{r_i}||} \cdot\frac{\partial ||\boldsymbol{r}_i||}{\partial \boldsymbol{x}}\&lt;br&gt;
= \sum_i \frac{\partial \rho (||\boldsymbol{r}_i||)}{\partial ||\boldsymbol{r_i}|| \cdot ||\boldsymbol{r}_i||} \cdot \boldsymbol{r}_i^T \frac{\partial ||\boldsymbol{r}_i||}{\partial \boldsymbol{x}}\&lt;br&gt;
= \sum_i \frac{\Psi (||\boldsymbol{r}_i||)}{||\boldsymbol{r}_i||} \cdot \boldsymbol{r}_i^T \frac{\partial ||\boldsymbol{r}_i||}{\partial \boldsymbol{x}} \ \ \ \ \ \ \ \&lt;br&gt;
= \sum_i w(||\boldsymbol{r}_i||) \cdot \boldsymbol{r}_i^T \frac{\partial ||\boldsymbol{r}_i||}{\partial \boldsymbol{x}} = 0 \ \ \tag{16}
$$&lt;/p&gt;
&lt;p&gt;其中 $\Psi (||\boldsymbol{r}_i||) = \frac{\partial \rho (||\boldsymbol{r}_i||)}{\partial ||\boldsymbol{r_i}||}$，称为&lt;code&gt;影响函数 (influence function)&lt;/code&gt;，$w(r) = \frac{\Psi(r)}{r}$ 称为&lt;code&gt;加权函数(weight function)&lt;/code&gt;。可以看出用公式(12)求解 (10)的极小值等于最小化&lt;code&gt;迭代重加权最小二乘 (Iteratively Reweighted Least Squares, IRLS)问题&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;$$
E_{IRLS} = \sum_i w(||\boldsymbol{r}_i||)||\boldsymbol{r}_i||^2 \tag{17}
$$&lt;/p&gt;
&lt;p&gt;其中 $w(||\boldsymbol{r}_i||)$ 起着局部加权作用。IRLS 算法在计算影响函数 $w(||\boldsymbol{r}_i||)$ 和求解得到的加权最小二乘问题(固定的 $w$ 值) 之间交替。M-estimator 一定能够减少外点的影响，但是在一些情况中，从太多外点起步的话会使得 IRLS(或者其他梯度下降算法) 不能收敛到全局最优。&lt;/p&gt;
&lt;h2 id=&#34;4-优化问题在三维视觉中的应用&#34;&gt;4. 优化问题在三维视觉中的应用&lt;/h2&gt;
&lt;h3 id=&#34;41-rotation-averaging&#34;&gt;4.1 Rotation Averaging&lt;/h3&gt;
&lt;h4 id=&#34;411-相对运动和全局姿态的约束关系&#34;&gt;4.1.1 相对运动和全局姿态的约束关系&lt;/h4&gt;
&lt;p&gt;Rotation Averaging 是 Structure from Motion (SfM) 的一个子问题，现在在SLAM方面也有应用。无论是SfM还是SLAM，都需要估计摄像机的姿态 (pose)，摄像机的姿态由旋转 (rotation，或是朝向 orientation) 和平移 (translation，也可以说是摄像机位置)构成。&lt;/p&gt;
&lt;p&gt;问题描述为: 给定 $N$ 张图片，全局运动可以用 $N-1$ 个运动模型描述。两个相机之间的相对运动关系可以通过对极几何估计&lt;code&gt;本质矩阵(Essential Matrix)&lt;/code&gt;，再进行矩阵分解(对本质矩阵的奇异值分解)得到。我们可以通过相对相机运动来估计全局相机运动模型。如果每对图片之间都有足够的重叠，那么我们可以得到 $\frac{N(N-1)}{2}$ 对约束(虽然实际上不可能存在这么多对，但一般也存在远多于 $N$ 的约束)。&lt;/p&gt;
&lt;p&gt;为了求解 RA 问题，首先来推几个公式。假设已知两个相机在全局坐标系下的运动，那么这两个相机之间的相对运动是怎样的关系呢？&lt;/p&gt;
&lt;p&gt;已知点 $P$ 在世界坐标系中的坐标，记为 $P_0$。对于两个不同的相机 $i$ 和 $j$，对应于全局坐标系中的旋转和平移分别为 $R_i, t_i, R_j, t_j$ 。将点 $P_0$ 分别投影到相机 $i$ 和 $j$ 的坐标系，那么有&lt;/p&gt;
&lt;p&gt;$$
P_i = R_iP_0 + t_j\ (18.a) \
P_j = R_jP_0 + t_j\ (18.b)
$$&lt;/p&gt;
&lt;p&gt;由公式(18.a)我们可以得到&lt;/p&gt;
&lt;p&gt;$$P_0 = R_i^{-1}(P_i - t_i)\ \tag{18} $$&lt;/p&gt;
&lt;p&gt;将公式(19)代入(18.b)中，有
$$
P_j = R_j(R_i^{-1}(P_i - t_i)) + t_j\&lt;br&gt;
\ = R_jR_i^{-1}P_i + (t_j - R_jR_i^{-1}t_i)\ \tag{20}
$$
由公式(19)我们可以得到相机 $i$ 到相机 $j$ 的相对运动关系:
$$
R_{ij} = R_jR_i^{-1} = R_jR_i^{T}\ (21.a)\&lt;br&gt;
t_{ij} = t_j - R_jR_i^{-1}t_i = t_j - R_{ij}t_i\ (21.b)
$$
这里需要注意的是 ***$R_{ij}$ 和 $t_{ij}$是相机 $i$ 相对于 $j$ 的运动*** 。&lt;/p&gt;
&lt;h4 id=&#34;412-ra-问题的优化求解&#34;&gt;4.1.2 RA 问题的优化求解&lt;/h4&gt;
&lt;p&gt;给定参考系和一系列相对旋转 ${R_{ij}}$，我们希望求解 $R_{global} = {R_1, \cdots, R_N}$。我们希望最小化代价函数:
$$
\arg\min_{R_i} \sum_{(i,j)\in \mathcal{E}} d^2(R_{ij}, R_jR_i^{-1}) \tag{22}
$$
现在我们考虑使用李代数来进行优化, $R_{ij} = exp([\boldsymbol{w_{ij}}]_{\times}), R_i = exp([\boldsymbol{w_{i}}]_{\times})$，其中 $\boldsymbol{w_{ij}}$ 和 $\boldsymbol{w_{i}}$ 分别为 $R_{ij}$ 和 $R_i$ 对应的李代数。&lt;/p&gt;
&lt;p&gt;假设只考虑其中一对关系 $R_{ij} = R_jR_i^{-1}$。由 BCH 公式： $BCH(x, y) = x + y + \frac{1}{2}[x, y] + \frac{1}{12}[x - y, [x, y]] + o(|(x, y)|^4)$，若只采用BCH公式的一阶估计 $BCH(x, y) \approx x + y$，则&lt;/p&gt;
&lt;p&gt;$$
\boldsymbol{w_{ij}} = BCH(\boldsymbol{w_{j}}, -\boldsymbol{w_{i}}) = \boldsymbol{w_{j}} - \boldsymbol{w_{i}}
$$&lt;/p&gt;
&lt;p&gt;令全局坐标系下旋转的李代数表示为 $\boldsymbol{\omega}_{global} = [\boldsymbol{\omega}_1, \cdots, \boldsymbol{\omega}_N]^T$，那么我们有&lt;/p&gt;
&lt;p&gt;$$
\boldsymbol{w_{ij}} = \boldsymbol{w_{j}} - \boldsymbol{w_{i}} =
\left[
\begin{array}{cccc}
\cdots &amp;amp; -I &amp;amp; \cdots &amp;amp; I &amp;amp; \cdots
\end{array}
\right]
\boldsymbol{w_{global}}
= A_{ij}\boldsymbol{w_{global}} \tag{23}
$$&lt;/p&gt;
&lt;p&gt;在 $A_{ij}$ 中，$I$ 和 $_I$ 为在 $i$ 和 $j$ 处的 $3\times 3$ 的块矩阵。把所有的相对运动关系拼合起来，我们有
$$
A\boldsymbol{w_{global}} = \boldsymbol{w_{rel}} \tag{24}
$$&lt;/p&gt;
&lt;p&gt;现在，我们要做的就是如何求解这个非齐次线性方程组了。&lt;/p&gt;
&lt;h5 id=&#34;1-l_1-rotation-averaging&#34;&gt;(1) $l_1$ Rotation Averaging&lt;/h5&gt;
&lt;p&gt;考虑非齐次线性方程组 $A\boldsymbol{x} = \boldsymbol{b}$，其中 $\boldsymbol{x} \in \mathbb{R}^n, \boldsymbol{b} \in \mathbb{R}^m$，并且 $m &amp;gt; n$。这个方程的求解难度由于噪声和外点的存在发生了改变，比如 $\boldsymbol{b} = A\boldsymbol{x} + \boldsymbol{e} $。在压缩感知方向的一些研究表明，在外点存在的情况下，$\boldsymbol{x}$ 可以通过以下公式高效精确地求解:&lt;/p&gt;
&lt;p&gt;$$
\arg\min_x ||A\boldsymbol{x} - \boldsymbol{b}||_{l_1} \tag{25}
$$&lt;/p&gt;
&lt;h5 id=&#34;2-irls-rotation-averaging&#34;&gt;(2) IRLS Rotation Averaging&lt;/h5&gt;
&lt;p&gt;$l_1$优化得到的结果还能进一步提升。现在考虑使用 IRLS 算法来求解 Rotation Averaging 问题，令残差为 $\boldsymbol{e} = A\boldsymbol{x} - \boldsymbol{b}$，我们希望最小化鲁棒代价函数&lt;/p&gt;
&lt;p&gt;$$
E_{RLS} = \sum_i \rho(||\boldsymbol{e}_i||) \tag{26}
$$&lt;/p&gt;
&lt;p&gt;这里，loss function 选则 Huber-like loss function: $\rho(x) = \frac{x^2}{x^2 + \sigma^2}$。因此，&lt;/p&gt;
&lt;p&gt;$$
min\ E_{RLS} = min\ \sum_i \rho(||\boldsymbol{e}_i||) = min\ \sum_i \frac{\boldsymbol{e}_i^2}{\boldsymbol{e}_i^2 + \sigma^2} \tag{27}
$$&lt;/p&gt;
&lt;p&gt;公式(27)对 $\boldsymbol{x}$ 求偏导，有&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial E_{RLS}}{\partial \boldsymbol{x}} = \frac{\partial E_{RLS}}{\partial \boldsymbol{e}} \cdot \frac{\partial \boldsymbol{e}}{\partial \boldsymbol{x}} \qquad \qquad \qquad \qquad \ \&lt;br&gt;
= \frac{2||\boldsymbol{e}_i||(||\boldsymbol{e}_i||^2 + \sigma^2) - 2||\boldsymbol{e}_i||\cdot ||\boldsymbol{e}_i||^2}{(||\boldsymbol{e}_i||^2 + \sigma^2)^2} \cdot A^T\&lt;br&gt;
= \frac{2||\boldsymbol{e}_i||\sigma^2}{(||\boldsymbol{e}_i||^2 + \sigma^2)^2} \cdot A^T \qquad \qquad \qquad \qquad \quad \&lt;br&gt;
= \frac{2(A\boldsymbol{x}-b)\sigma^2 A^T}{(||\boldsymbol{e}_i||^2 + \sigma^2)^2} \qquad \qquad \qquad \qquad \qquad \&lt;br&gt;
= 2 \cdot \frac{A^T \sigma^2A\boldsymbol{x}- A^T\sigma^2b}{(||\boldsymbol{e}_i||^2 + \sigma^2)^2} = 0 \qquad \qquad \qquad\  \tag{28}
$$&lt;/p&gt;
&lt;p&gt;因此，有&lt;/p&gt;
&lt;p&gt;$$
A^T\Phi(\boldsymbol{e})A\boldsymbol{x} - A^T\Phi(\boldsymbol{e}) \boldsymbol{b} = 0\ \Rightarrow \&lt;br&gt;
A^T\Phi(\boldsymbol{e})A\boldsymbol{x} = A^T\Phi(\boldsymbol{e}) \boldsymbol{b} \tag{29}
$$&lt;/p&gt;
&lt;p&gt;其中，$\Phi(\boldsymbol{e})$ 是对角阵并且 $\Phi(i, i) = \frac{\sigma^2}{(\boldsymbol{e}_i^2 + \sigma^2)^2}$ 。&lt;/p&gt;
&lt;p&gt;由公式 (17), RLS 问题可以转变为求解 IRLS 问题： $min\ (A\boldsymbol{x} - \boldsymbol{b})^T\Phi(A\boldsymbol{x} - \boldsymbol{b})$，由公式 (29) 知，其最优点为&lt;/p&gt;
&lt;p&gt;$$\boldsymbol{x} = (A^T \Phi A)^{-1} A^T \Phi \boldsymbol{b} \tag{30}$$&lt;/p&gt;
&lt;p&gt;在 IRLS 中，我们先固定$\boldsymbol{x}$，然后计算 $\Phi$；接着固定 $\Phi$ (由公式 (30) 计算得到)。通过这样一个交替的方式直到收敛。&lt;/p&gt;
&lt;p&gt;如果没有一个好的初值的话，&lt;code&gt;IRLS&lt;/code&gt; 算法很有可能不能收敛到全局最优，而由于 &lt;code&gt;L1RA&lt;/code&gt; 算法对于 $R_{gobal}$ 来说足够提供一个好的初值，因此 &lt;code&gt;L1RA&lt;/code&gt; 算法的输出可以作为 &lt;code&gt;IRLS&lt;/code&gt; 算法的输入。到这里，我们可以直到，整个使用 $l_1$-范数求解 Rotion Averaging 问题的算法(L1-IRLS)可以分为两步: (1) &lt;code&gt;L1RA&lt;/code&gt;算法求解 $R_{global}$ 的初值；（2）$R_{global}$ 的初值作为 &lt;code&gt;IRLS&lt;/code&gt; 的输入，使用算法2求解得到最终的 $R_{global}$ 。&lt;/p&gt;
&lt;h3 id=&#34;42-光束平差法-bundle-adjustment&#34;&gt;4.2 光束平差法 (Bundle Adjustment)&lt;/h3&gt;
&lt;h4 id=&#34;421-相机投影模型&#34;&gt;4.2.1 相机投影模型&lt;/h4&gt;
&lt;p&gt;为了理解 Bundle Adjustment，首先需要了解空间中的三维点重投影回图像平面的过程。如下图所示，$p$ 是空间中的三维点，$o-xyz$ 是相机坐标系，$o‘-x’y‘$ 是成像平面，成像平面内的蓝色区域是像素平面(图像平面)，$f$ 是焦距。对于世界坐标系中的三维点，首先要通过相机在世界坐标系中的旋转和平移变换到相机坐标系 ($x = RX + t$)，进行归一化后，对归一化后的点进行去畸变，最后通过内参矩阵 $K$ 投影到图像平面。这一过程通过函数 $\pi(C_j, X_i)$ 描述。&lt;/p&gt;
&lt;div align=center&gt;
    &lt;img src=&#34;https://AIBluefisher.github.io/img/optimization/pinhole_camera_model1.jpg&#34;/&gt;
&lt;/div&gt;
&lt;h4 id=&#34;422-ba-求解&#34;&gt;4.2.2 BA 求解&lt;/h4&gt;
&lt;p&gt;BA 最小化重投影误差&lt;/p&gt;
&lt;p&gt;$$
min\ \sum_{i=1}^{n} \sum_{j=1}^m ||u_{ij} - \pi(C_j, X_i)||_2^2 \tag{31}
$$&lt;/p&gt;
&lt;p&gt;其中 $n$ 为三维点的个数，$m$ 为相机个数。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://AIBluefisher.github.io/img/optimization/bundle_adjustment1.jpg&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;令残差 $r_{ij} = u_{ij} - \pi(C_j, X_i)$，公式 (31) 可重写为&lt;/p&gt;
&lt;p&gt;$$
\min\ r^Tr \tag{32}
$$&lt;/p&gt;
&lt;p&gt;对 $r$ 进行泰勒展开&lt;/p&gt;
&lt;p&gt;$$
r(x + \delta x) = r(x) + g^T \delta x + \frac{1}{2}\delta x^T H \delta x + o(||\delta x||^2)\tag{33}
$$&lt;/p&gt;
&lt;p&gt;(33) 右边对 $\delta x$ 求导，并令其等于０，可以得到&lt;/p&gt;
&lt;p&gt;$$
H\delta x = -g \tag{34}
$$&lt;/p&gt;
&lt;p&gt;公式 (34) 其实就是&lt;code&gt;牛顿方程 (Newton Equation)&lt;/code&gt;。对于最小二乘问题来说， $H = J^T J + S, g = J^Tr$，其中 $S = \sum_{i=1}^n \sum_{j=1}^m r_{ij}\nabla^2 r_{ij}$。如果残差足够小，那么我们可以忽略 $S$。这个时候我们就可以得到高斯牛顿  (Gauss-Newton) 方程&lt;/p&gt;
&lt;p&gt;$$
J^TJ \delta x = -J^Tr \tag{35}
$$&lt;/p&gt;
&lt;p&gt;由 (35) 可知，当 $J$ 满秩，$g$ 不为0 的时候，$\delta x$ 一定是下降方向，因为 $(\delta x)^T g = (\delta x)^T J^T r = -(\delta x)^T J^T J \delta x = -||J \delta x||^2 &amp;lt; 0$。但是当 $J^TJ$ 奇异的时候，高斯牛顿法在数值上会变得不稳定。这个时候，我们可以用 Levenberg-Marquardt 方法来代替&lt;/p&gt;
&lt;p&gt;$$
(J^TJ + \lambda I)\delta x = -J^T r \tag{36}
$$&lt;/p&gt;
&lt;p&gt;对于 Bundle Adjustment 问题，似乎我们只需要用线性代数的知识，就能够很容易地求解 (35) 和 (36) 了。但事实上真的是这样吗？&lt;/p&gt;
&lt;p&gt;在三维重建中，我们可能需要优化非常多的相机和三维点，SfM 里优化几千个相机以及几百万甚至上千万个三维点的情形不在少数。这种情况下，Jacobian 矩阵的存储就是一大问题，除此之外，解这么大的一个方程组也会变得非常耗时。有没有什么方法能够让求解变得更快呢？我们先来分析分析Jocobian矩阵的结构。&lt;/p&gt;
&lt;p&gt;令 $\hat{u}_{ij} = \pi(C_j, X_i)$，其中需要优化的变量 $x$ 由相机参数 $C_j$ 和 三维点 $X_i$ 组成，因此我们把 $x$ 分成相机参数块 $c$ 和三维结构参数块 $p$&lt;/p&gt;
&lt;p&gt;$$
x = [c\ p] \tag{37}
$$&lt;/p&gt;
&lt;p&gt;由于&lt;/p&gt;
&lt;p&gt;$$
J_{ij} = \frac{\partial r_{ij}}{\partial x_k} = \frac{\partial \hat{u}_{ij}}{\partial x_k}
$$&lt;/p&gt;
&lt;p&gt;我们很容易发现:&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial \hat{u}&lt;em&gt;{ij}}{\partial c_k} = 0, \forall j \neq k,
\frac{\partial \hat{u}&lt;/em&gt;{ij}}{\partial p_k} = 0, \forall i \neq k
$$&lt;/p&gt;
&lt;p&gt;为了方便起见，现在我们考虑上图只有3个相机和4个三维点的情况，其中每个相机都能观测到这4个三维点。令 $A_{ij} = \frac{\partial \hat{u}_{ij}}{\partial c_j}, B_{ij} = \frac{\partial \hat{u}_{ij}}{\partial p_j}$，我们可以求得 Jacobian 矩阵:&lt;/p&gt;
&lt;p&gt;$$
J =
\left[
\begin{array}{ccccccc}
A_{11} &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{11} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
0 &amp;amp; A_{12} &amp;amp; 0 &amp;amp; B_{12} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; A_{13} &amp;amp; B_{13} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
A_{21} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{21} &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
0 &amp;amp; A_{22} &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{22} &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; A_{23} &amp;amp; 0 &amp;amp; B_{23} &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
A_{31} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{31} &amp;amp; 0\&lt;br&gt;
0 &amp;amp; A_{32} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{32} &amp;amp; 0\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; A_{33} &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{33} &amp;amp; 0\&lt;br&gt;
A_{41} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{41}\&lt;br&gt;
0 &amp;amp; A_{42} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{42}\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; A_{43} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; B_{43}
\end{array}
\right] \tag{38}
$$&lt;/p&gt;
&lt;p&gt;现在我们可以发现，BA 中的 Jacobian 是一个稀疏的矩阵，它包含了大量的零元素。那么，对于高斯牛顿方程和LM方法里的 $J^TJ$，我们也不难发现它的稀疏性。令&lt;/p&gt;
&lt;p&gt;$$
U_j = \sum_{i=1}^4 A_{ij}^T A_{ij}, V_i = \sum_{j=1} B_{ij}^T B_{ij}, W_{ij} = A_{ij}^T B_{ij} \tag{39}
$$&lt;/p&gt;
&lt;p&gt;$$
J^T J =
\left[
\begin{array}{ccccccc}
U_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; W_{11} &amp;amp; W_{21} &amp;amp; W_{31} &amp;amp; W_{41}\&lt;br&gt;
0 &amp;amp; U_2 &amp;amp; 0 &amp;amp; W_{12} &amp;amp; W_{22} &amp;amp; W_{32} &amp;amp; W_{42}\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; U_3 &amp;amp; W_{13} &amp;amp; W_{23} &amp;amp; W_{33} &amp;amp; W_{43}\&lt;br&gt;
W_{11}^T &amp;amp; W_{12}^T &amp;amp; W_{13}^T &amp;amp; V_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
W_{21}^T &amp;amp; W_{22}^T &amp;amp; W_{23}^T &amp;amp; 0 &amp;amp; V_2 &amp;amp; 0 &amp;amp; 0\&lt;br&gt;
W_{31}^T &amp;amp; W_{32}^T &amp;amp; W_{33}^T &amp;amp; 0 &amp;amp; 0 &amp;amp; V_3 &amp;amp; 0\&lt;br&gt;
W_{41}^T &amp;amp; W_{42}^T &amp;amp; W_{43}^T &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; V_4
\end{array}
\right]
\tag{40}
$$&lt;/p&gt;
&lt;p&gt;现在，我们可以看到 Ba 中 Hessian 的近似 $J^TJ$ 也是一个稀疏的矩阵。对于更大规模的情况，Hessian的稀疏性如下图所示(注意这张图里的相机和三维点的顺序不一样):&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;img src = &#34;/img/optimization/hessian.png&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;对于 (36) 的右边，有&lt;/p&gt;
&lt;p&gt;$$
J^T r =
\left(
\begin{array}{c}
\sum_{i=1}^4(A_{i1}^T r_{i1})\&lt;br&gt;
\sum_{i=1}^4(A_{i2}^T r_{i2})\
\sum_{i=1}^4(A_{i3}^T r_{i3})\
\sum_{j=1}^3 (B_{ij}^T r_{ij})\
\sum_{j=1}^3 (B_{ij}^T r_{2j})\
\sum_{j=1}^3 (B_{ij}^T r_{3j})\
\sum_{j=1}^3 (B_{ij}^T r_{4j})
\end{array}
\right)
\tag{41}
$$&lt;/p&gt;
&lt;p&gt;令&lt;/p&gt;
&lt;p&gt;$$
r_{c_j} = \sum_{i=1}^4(A_{ij}^T r_{ij})^T, r_{p_i} = \sum_{j=1}^3(B_{ij}^T r_{ij})^T,
r_{ij} = u_{ij} - \hat{u_{ij}}
\tag{42}
$$&lt;/p&gt;
&lt;p&gt;并且&lt;/p&gt;
&lt;p&gt;$$
U = diag{U_1, U_2, U_3},
V = diag{V_1, V_2, V_3, V_4},
\tag{43}
$$&lt;/p&gt;
&lt;p&gt;$$
W = \left[
\begin{array}{cccc}
W_{11} &amp;amp; W_{21} &amp;amp; W_{31} &amp;amp; W_{41} \&lt;br&gt;
W_{12} &amp;amp; W_{22} &amp;amp; W_{32} &amp;amp; W_{42} \&lt;br&gt;
W_{13} &amp;amp; W_{23} &amp;amp; W_{33} &amp;amp; W_{43}
\end{array}
\right]
\tag{44}
$$&lt;/p&gt;
&lt;p&gt;我们把 (42)、(43)、(44) 代入 (35) 中，并且令 $\delta x = (\delta c\ \delta p)^T$，可以得到&lt;/p&gt;
&lt;p&gt;$$
\left[
\begin{array}{cc}
U &amp;amp; W  \&lt;br&gt;
W^T &amp;amp; V
\end{array}
\right]
\left[
\begin{array}{c}
\delta c \&lt;br&gt;
\delta p
\end{array}
\right] =
\left[
\begin{array}{c}
r_c \&lt;br&gt;
r_p
\end{array}
\right]
\tag{45}
$$&lt;/p&gt;
&lt;p&gt;现在，直接求解(45)在时间和空间上肯定是不可行的。注意到，(45) 中 $U, \delta c$ 是和相机相关的参数，$V,\delta p$ 是和三维点相关的参数，实际中的相机数远小于三维点的数量。因此，我们可以考虑先消去和三维点相关的参数，求得相机之后再代回求解三维点。现在，我们对 (45) 的左右两边同乘 $[I -WV^{-1}]^T$，可以得到&lt;/p&gt;
&lt;p&gt;$$
\left[
\begin{array}{cc}
U-WV^{-1}W^T &amp;amp; 0
\end{array}
\right]
\left[
\begin{array}{c}
\delta c \&lt;br&gt;
\delta p
\end{array}
\right] =
\left[
\begin{array}{cc}
I &amp;amp; -WV^{-1}  \&lt;br&gt;
\end{array}
\right]
\left[
\begin{array}{c}
r_c \&lt;br&gt;
r_p
\end{array}
\right]
\tag{46}
$$
进一步我们可以得到
$$
(U - WV^{-1}W^T)\delta c = r_c - WV^{-1}r_p \tag{47}
$$&lt;/p&gt;
&lt;p&gt;BA中，(47) 称作&lt;code&gt;Reduced Camera System&lt;/code&gt;，$S = U - WV^{-1}W^T$ 称作 $V$ 的 &lt;code&gt;Schur Complement&lt;/code&gt; 。可以证明，对称正定矩阵的 Schur Complement 是一个对称正定矩阵。因此，(47) 可以通过&lt;code&gt;乔里斯基分解 (Cholesky Decomposition)&lt;/code&gt; 进行求解。由 (46)，我们有&lt;/p&gt;
&lt;p&gt;$$
W^T\delta c + V\delta p = r_c \tag{48}
$$&lt;/p&gt;
&lt;p&gt;在求解得到相机之后，由 (48)，我们有&lt;/p&gt;
&lt;p&gt;$$
\delta p = V^{-1}(r_p - W^T\delta c) \tag{49}
$$&lt;/p&gt;
&lt;p&gt;到这里，BA 的求解基本上可以结束了。不过，为了加速BA，还有一些求解技巧，像 &lt;code&gt;Preconditioned Conjugate Gradient&lt;/code&gt;。对这些感兴趣的可以参考相关文献 [6,7,8]。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;[1] Boyd. Convex Optimization.&lt;/p&gt;
&lt;p&gt;[2] Govindu V M . Combining two-view constraints for motion estimation[C]// Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. IEEE, 2001.&lt;/p&gt;
&lt;p&gt;[3] Govindu V M . Lie-Algebraic Averaging for Globally Consistent Motion Estimation[C]// null. IEEE Computer Society, 2004.&lt;/p&gt;
&lt;p&gt;[4] Chatterjee A , Govindu V M . Efficient and Robust Large-Scale Rotation Averaging[C]// 2013 IEEE International Conference on Computer Vision (ICCV). IEEE Computer Society, 2013.&lt;/p&gt;
&lt;p&gt;[5] Chatterjee A, Govindu V. Robust Relative Rotation Averaging[J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, PP(99):1-1.&lt;/p&gt;
&lt;p&gt;[6] Lourakis M I A, Argyros A A. SBA: a software package for generic sparse bundle adjustment[J]. Acm Trans.math.softw, 2009, 36(1):2.&lt;/p&gt;
&lt;p&gt;[7] Triggs B, Mclauchlan P F, Hartley R I, et al. Bundle Adjustment — A Modern Synthesis[C]// International Workshop on Vision Algorithms: Theory and Practice. Springer-Verlag, 1999:298-372.&lt;/p&gt;
&lt;p&gt;[8] Jeong Y, Nistér D, Steedly D, et al. Pushing the envelope of modern methods for bundle adjustment.[J]. IEEE Trans Pattern Anal Mach Intell, 2012, 34(8):1605-1617.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Survey of Global Structure from Motion</title>
      <link>https://AIBluefisher.github.io/post/globalsfm/</link>
      <pubDate>Wed, 12 Jun 2019 22:23:14 +0800</pubDate>
      <guid>https://AIBluefisher.github.io/post/globalsfm/</guid>
      <description>&lt;h2 id=&#34;1-global-structure-from-motion-by-similarity-averaging&#34;&gt;1. Global Structure-from-Motion by Similarity Averaging&lt;/h2&gt;
&lt;p&gt;之前所有的global SfM方法都分为四步：Rotation Averaging, Translation Averaging, Triangulation, Bundle Adjustment。但是，众所周知的是，translation averaging这一步很难做好，主要有两个原因: essential matrix只能估计的relative translation只有方向，没有尺度；很难剔除所有的outlier导致的bad essential matrix。这篇文章的方法则拓宽了思路，如果知道尺度的话，那么global translation的估计就容易的多了。为此，这篇文章通过卫星图构建的局部重建为每张图片生成一张稀疏深度图，通过稀疏深度图来估计图片之间相对尺度。&lt;/p&gt;
&lt;h3 id=&#34;11-方法概述&#34;&gt;1.1 方法概述&lt;/h3&gt;
&lt;p&gt;这篇文章的pipeline如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_1.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;首先通过特征点匹配之后得到的correspondences计算essential matrix，由于essential matrix只能在parallel rigid graph中确定相机位置，因此这篇文章通过从stellate graph构造为每张图片都构造一张深度图来提升global SfM方法。stellate graph如下图(b)所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_2.png&#34; alt=&#34;stellate graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;在构造稀疏深度图的时候还会通过depth consistency check来去除noisy essential matrices。除此之外，还有两个可选的筛除outlier的步骤: &lt;code&gt;local BA&lt;/code&gt;用于local stellate graph来提升relative motion的准确性并且筛除一些错误的essential matrices；&lt;code&gt;missing correspondence analysis&lt;/code&gt;用于排除由于重复的场景结构造成的outlier essential matrices。之后，Rotation Averaging和Scale Averaging同时进行。一旦global rotation和global scale都确定了，scale-aware translation averaging(尺度已知的translation averaging)就可以用于估计global scale了。最后，triangulation之后通过BA来同时优化相机姿态和三维点。&lt;/p&gt;
&lt;h3 id=&#34;12-稀疏深度图的构建&#34;&gt;1.2 稀疏深度图的构建&lt;/h3&gt;
&lt;p&gt;对于epipolar graph中的每一条边，我们都已经通过triangulation计算得到一个局部重建，基线宽度设为1。之后，可以通过一个卫星图将这些局部重建拼接起来。这个卫星图不包括环，因此这一步骤简单且鲁棒。为了考虑计算效率，只最多考虑和$i$相连的80个相机。这些相机之间的边是匹配数最多的。对于卫星图之间的局部重建，只需要对图片(i, j)之间计算一个相对尺度$s_{ij}^i$就能将这些局部重建拼接起来。这里的上标$i$表示以$i$为卫星图的中心。对于图片$i$中的一个特征点，如果它同时在$(i, j)$和$(i, k)$中都重建出来了，他的深度和这两个局部重建之间的尺度关系为:&lt;/p&gt;
&lt;p&gt;$$
\frac{s_{ik}^i}{s_{ij}^i} = \frac{d_{ij}}{d_{ik}} = d_{jk}^k \tag{1}
$$&lt;/p&gt;
&lt;p&gt;其中，$d_{ij}$ 和 $d_{ik}$分别是特征点从$(i, j)$和$(i, k)$中重建出来的。对公式(1)的两边同时取对数，则有&lt;/p&gt;
&lt;p&gt;$$
log(s_{ik}^i) - log(s_{ij}^i) = log(d_{jk}^i) \tag{2}
$$&lt;/p&gt;
&lt;p&gt;把所有这些线性方程收集起来，可以得到一个线性方程组
$$
Ax=b \tag{3}
$$
其中 $x$ 和 $b$ 分别是把 $log(s_{ij}^i)$ 和 $log(d_{jk}^i)$ 堆叠在一起的向量。为了对(3)进行鲁棒估计，可以通过在 $L_1$-范数意义下进行优化:&lt;/p&gt;
&lt;p&gt;$$
\arg \min_x ||Ax-b||_1 \tag{4}
$$
$L_1$优化是凸函数可以得到全局最优解，可以通过ADMM进行求解。&lt;/p&gt;
&lt;h4 id=&#34;121-depth-consistency-check&#34;&gt;1.2.1 Depth consistency check&lt;/h4&gt;
&lt;p&gt;将这些局部重建合并在一起会造成 $i$ 中的每个特征点有多个深度值。可以通过检验它们的一致性来筛选outlier essential matrices。所有深度值偏离中值滤波之后的深度值的5%都被当做outlier。如果图片对$(i, j)$中的inlier少于5，那么这个图片对将被移除，因为一个本质矩阵需要至少5个点来计算得到。&lt;/p&gt;
&lt;h4 id=&#34;122-missing-correspondence-analysis&#34;&gt;1.2.2 missing correspondence analysis&lt;/h4&gt;
&lt;p&gt;missing correspondence analysis已经在一些文献中证明，对于有large repetitive scene structures是一种有效的筛除outlier EG的策略。这篇文章将深度图$D_i$中的三维点都投影到$j$的图像平面中，如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_3.png&#34; alt=&#34;FOV&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中绿色的点表示在$j$中匹配上的特征点。红色的点是&lt;code&gt;missing correspondences&lt;/code&gt;，也就是在$j$中没有匹配上的点。可以通过分析红色点和绿色点的比例来移除repetitive scene structures中的outlier epipolar graph。绿虚线框中的红点是由于feature matching的imperfect repeatability造成的，因此，这篇文章只考虑框外的红色点。对于$M_j^i = \frac{n_{red}}{n_{green}}$，如果$M_j^i &amp;gt; \epsilon$，$(i, j)$就被当成outlier被移除掉。&lt;/p&gt;
&lt;h3 id=&#34;13-similarity-averaging&#34;&gt;1.3 Similarity Averaging&lt;/h3&gt;
&lt;p&gt;一旦深度图构建完成，对于epipolar graph 中的每一条边，都可以计算一个相似变换。理论上，相似变换可以通过3D-3D的对应计算得到，但是实际上，这些local reconstructions用来做3D-3D的registeration是不够准的(这也验证了我之前的实验，ransac采了很多次样都算不出一个准确的similarity transformation)。因此，对于相似变换中的R和t，可以从之前得到的essential matrices计算得到或者local BA之后得到，并且通过以下公式计算相对尺度：&lt;/p&gt;
&lt;p&gt;$$
S_{ij} = s_{ji}^j / s_{ij}^i \tag{5}
$$
其中，$s_{ji}^j$和$s_{ij}^i$是在构建$D_i$和$D_j$时通过(4)解出来的相对尺度。&lt;/p&gt;
&lt;h4 id=&#34;131-robust-scale-averaging&#34;&gt;1.3.1 Robust Scale Averaging&lt;/h4&gt;
&lt;p&gt;对于每张深度图$D_i$，我们都需要计算一个尺度因子$s_i$来把它们register到一起。由已知的relative scale，我们有&lt;/p&gt;
&lt;p&gt;$$
\frac{s_i}{s_j} = S_{ij} \tag{6}
$$&lt;/p&gt;
&lt;p&gt;和(4)类似，我们可以得到&lt;/p&gt;
&lt;p&gt;$$
A_sx_s=b_s \tag{7}
$$&lt;/p&gt;
&lt;p&gt;同样，我们可以在$L_1$-范数下求解这个线性方程:&lt;/p&gt;
&lt;p&gt;$$
\arg \min_{x_s} ||A_sx_s-b_s||_1 \tag{8}
$$&lt;/p&gt;
&lt;h4 id=&#34;132-robust-scale-aware-translation-averaging&#34;&gt;1.3.2 Robust Scale-Aware Translation Averaging&lt;/h4&gt;
&lt;p&gt;当每张深度图的全局尺度因子都已知时，基线宽度可以通过以下公式计算得到:&lt;/p&gt;
&lt;p&gt;$$
b_{ij} = \frac{1}{2}(s_i s_{ij}^i + s_j s_{ij}^j) \tag{9}
$$
在global rotation已知之后，可以通过以下线性方程计算相机位置:&lt;/p&gt;
&lt;p&gt;$$
R_j(c_i - c_j) = b_{ij}t_{ij} \tag{9}
$$&lt;/p&gt;
&lt;p&gt;这些方程收集起来同样可以得到一个线性方程组:&lt;/p&gt;
&lt;p&gt;$$
A_cx_c = b_c \tag{10}
$$&lt;/p&gt;
&lt;p&gt;然后通过优化$L_1$-范数下的方程:&lt;/p&gt;
&lt;p&gt;$$
\arg \min_{x_c} ||A_c x_c - b_c||_1 \tag{11}
$$&lt;/p&gt;
&lt;h2 id=&#34;2-实验&#34;&gt;2. 实验&lt;/h2&gt;
&lt;h3 id=&#34;21-重复场景的数据集&#34;&gt;2.1 重复场景的数据集&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_4.png&#34; alt=&#34;global_sim_4&#34;&gt;&lt;/p&gt;
&lt;p&gt;(a)为数据集中的图片，(b)为1DSfM的结果，(c)为这篇文章的结果&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_5.png&#34; alt=&#34;global_sim_5&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-sequential-dataset&#34;&gt;2.2 Sequential Dataset&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_6.png&#34; alt=&#34;global_sim_6&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-internet-dataset&#34;&gt;2.3 Internet Dataset&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_7.png&#34; alt=&#34;global_sim_7&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;24-统计结果&#34;&gt;2.4 统计结果&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/globalsfm/global_sim_8.png&#34; alt=&#34;global_sim_8&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Moulon P , Monasse P , Marlet R . Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion[C]// IEEE International Conference on Computer Vision. IEEE Computer Society, 2013.&lt;/p&gt;
&lt;p&gt;[2] Cui Z , Tan P . Global Structure-from-Motion by Similarity Averaging[C]// 2015 IEEE International Conference on Computer Vision (ICCV). IEEE, 2015.&lt;/p&gt;
&lt;p&gt;[3] Sweeney C , Sattler T , Hollerer T , et al. Optimizing the Viewing Graph for Structure-from-Motion[C]// 2015 IEEE International Conference on Computer Vision (ICCV). IEEE Computer Society, 2015.&lt;/p&gt;
&lt;p&gt;[4] Sengupta S , Amir T , Galun M , et al. A New Rank Constraint on Multi-view Fundamental Matrices, and its Application to Camera Location Recovery[J]. 2017.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>三维重建中的旋转(Rotation)</title>
      <link>https://AIBluefisher.github.io/post/rotation/</link>
      <pubDate>Tue, 16 Apr 2019 22:23:14 +0800</pubDate>
      <guid>https://AIBluefisher.github.io/post/rotation/</guid>
      <description>&lt;p&gt;旋转在三维重建中是比较重要的，这里主要对旋转的性质及应用做一些总结。&lt;/p&gt;
&lt;h3 id=&#34;1-旋转矩阵&#34;&gt;1. 旋转矩阵&lt;/h3&gt;
&lt;p&gt;设某个&lt;u&gt;单位正交基&lt;/u&gt; $(e_1, e_2, e_3)$ 经过一次旋转变成了 $(e_1^{&#39;}, e_2^{&#39;}, e_3^{&#39;})$ 。那么，对于同一个向量 $\boldsymbol{a}$ (注意该向量并没有随着坐标系的旋转而发生运动)，它在两个坐标系下的坐标为 $(a_1, a_2, a_3)^T$ 和 $(a_1^{&#39;}, a_2^{&#39;}, a_3^{&#39;})^T$。&lt;/p&gt;
&lt;p&gt;由坐标的定义，有：
$$
\left[
\begin{array}{ccc}
e_1 &amp;amp; e_2 &amp;amp; e_3
\end{array}
\right]
\left[
\begin{array}{c}
a_1 \ a_2 \ a_3
\end{array}
\right]=
\left[
\begin{array}{ccc}
e_1^{&#39;} &amp;amp; e_2^{&#39;} &amp;amp; e_3^{&#39;}
\end{array}
\right]
\left[
\begin{array}{c}
a_1^{&#39;} \ a_2^{&#39;} \ a_3^{&#39;}
\end{array}
\right] \tag{1}
$$
为描述两个坐标之间的关系，(1)左右两边同时乘以$(e_1^T, e_2^T, e_3^T)^T$，则有
$$
\left[
\begin{array}{c}
a_1 \ a_2 \ a_3
\end{array}
\right]=
\left[
\begin{array}{ccc}
e_1^{T}e_1{&#39;} &amp;amp; e_1^Te_2^{&#39;} &amp;amp; e_1^Te_3^{&#39;} \&lt;br&gt;
e_2^{T}e_1^{&#39;} &amp;amp; e_2^Te_2^{&#39;} &amp;amp; e_1^Te_3^{&#39;} \&lt;br&gt;
e_3^{T}e_1^{&#39;} &amp;amp; e_3^Te_2^{&#39;} &amp;amp; e_3^Te_3^{&#39;}
\end{array}
\right]
\left[
\begin{array}{c}
a_1^{&#39;} \ a_2^{&#39;} \ a_3^{&#39;}
\end{array}
\right] = R\boldsymbol{a^{&#39;}} \tag{2}
$$
其中 $R$ 即为旋转矩阵。旋转矩阵的集合定义为:
$$
SO(n) = {R \in \mathbb{R}^{n\times n} | RR^T = I, det(R) = I } \tag{3}
$$
由于旋转矩阵是正交阵，它的逆(即转置)描述了一个相反的旋转，则有$\boldsymbol{a^{&#39;}} = R^{-1}\boldsymbol{a}=R^T\boldsymbol{a}$。显然，$R^T$刻画了一个相反的旋转。&lt;/p&gt;
&lt;h3 id=&#34;2-李群和李代数-lie-group-and-lie-algebra&#34;&gt;2. 李群和李代数 (Lie Group and Lie Algebra)&lt;/h3&gt;
&lt;p&gt;这里我们只描述旋转空间上的&lt;code&gt;李群&lt;/code&gt;和&lt;code&gt;李代数&lt;/code&gt;。
李群是指具有连续光滑性质的群。$SO(n)$在实数空间上是连续的。我们能够直观想象一个刚体能够连续地在空间中运动，所以他们都是李群。每个李群都有与之对应的李代数，李代数描述了李群的局部性质。旋转空间上的李群已经在公式(3)中做了描述。&lt;/p&gt;
&lt;h4 id=&#34;21-旋转空间上的李代数推导&#34;&gt;2.1 旋转空间上的李代数推导&lt;/h4&gt;
&lt;p&gt;设 $R$ 为某个相机的旋转，随时间连续变化，即 $R$ 为关于时间 $t$ 的函数 $R(t)$。由于 $R$ 为正交阵，则有
$$
R(t)R(t)^T = I \tag{4}
$$
等式两边对 $t$ 求导，则有 $\frac{\delta R(t)}{\delta t} R(t)^T + R(t)\frac{\delta R(t)^T}{\delta t} = 0$。整理可得，
$$
\frac{\delta R(t)}{\delta t} R(t)^T = -R(t)\frac{\delta R(t)^T}{\delta t} = -(\frac{\delta R(t)}{\delta t} R(t)^T)^T \tag{5}
$$
由(5)可以看出 $\frac{\delta R(t)}{\delta t} R(t)^T$ 是一个&lt;code&gt;反对称矩阵&lt;/code&gt; (反对称矩阵的定义: $A = -A^T$)。
而对于&lt;code&gt;反对称矩阵&lt;/code&gt;，我们总能找到一个三维向量 $\boldsymbol{w}$ 与之对应。一般地，$[\boldsymbol{w}]&lt;em&gt;{\times}$ 表示向量到反对称阵的变换。因此，我们有
$$
\frac{\delta R(t)}{\delta t} R^T = [\boldsymbol{w}]&lt;/em&gt;{\times} \tag{6}
$$
对公式(6), 左右两边同时右乘$R(t)$可得
$$
\frac{\delta R(t)}{\delta t} = [\boldsymbol{w}]&lt;em&gt;{\times} R(t) \tag{7}
$$
公式(7)是一个形如 $\frac{\mathrm{d}y}{\mathrm{d}x} = ay$ 的常微分方程，对方程两边同时去倒数，则有 $\frac{\mathrm{d}x}{\mathrm{d}y} = \frac{1}{ay}$。显然，$\mathrm{d}x = \frac{1}{y}\mathrm{d}y$ 的解为 $x = ln\ ay$，进一步可得 $y = e^{ax}$。将公式(7)代入，可得
$$
R(t) = e^{[\boldsymbol{w}]&lt;/em&gt;{\times}t} \tag{8}
$$&lt;/p&gt;
&lt;h4 id=&#34;22-李代数-mathfrakso3&#34;&gt;2.2 李代数 $\mathfrak{so}(3)$&lt;/h4&gt;
&lt;p&gt;李群 $SO(3)$ 对应的李代数是定义在 $R(3)$ 上的向量，记为 $\boldsymbol{w}$。每个 $\boldsymbol{w}$ 都可以生成一个反对称矩阵 $\Phi$ :
$$
\Phi = [\boldsymbol{w}]&lt;em&gt;{\times} =
\left[
\begin{array}{ccc}
0 &amp;amp; -w_3 &amp;amp; w_2\&lt;br&gt;
w_3 &amp;amp; 0 &amp;amp; -w_1\&lt;br&gt;
-w_2 &amp;amp; w_1 &amp;amp; 0
\end{array}
\right]
\in \mathbb{R}^{3\times 3}
$$
一般说，$\mathfrak{so}(3)$的元素是三维向量或者三维反对称矩阵，不加区别:
$$
\mathfrak{so}(3) = {\boldsymbol{w} \in R^3, \Phi = [\boldsymbol{w}]&lt;/em&gt;{\times} \in \mathbb{R}^{3\times 3}} \tag{9}
$$
至此，我们知道 $\mathfrak{so}(3)$ 是一个由三维向量组成的集合，每个向量对应到一个反对称矩阵，可以表达旋转矩阵的倒数。它与 $\mathfrak{so}(3)$ 的关系通过&lt;code&gt;指数映射 (exponential mapping)&lt;/code&gt;给定:
$$
R = exp([\boldsymbol{w}]_{\times}) \tag{10}
$$&lt;/p&gt;
&lt;h4 id=&#34;23-推导-mathfrakso3-上的指数映射&#34;&gt;2.3 推导 $\mathfrak{so}(3)$ 上的指数映射&lt;/h4&gt;
&lt;p&gt;由 公式 (10) 可知，它是一个矩阵的指数。在李群和李代数中，称为&lt;code&gt;指数映射 (exponential mapping)&lt;/code&gt;。
我们知道指数函数的&lt;code&gt;幂级数&lt;/code&gt;为
$$
exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}
$$
同样地，对于 $\mathfrak{so}(3)$ 中的任意元素 $[\boldsymbol{w}]_{\times}$，它的指数映射为
$$
exp([\boldsymbol{w}]_{\times}) = \sum_{n=0}^{\infty} \frac{([\boldsymbol{w}]_{\times})^n}{n!} \tag{11}
$$
令 $\boldsymbol{w} = \theta \boldsymbol{a}$，其中$\theta$ 为方向，$\boldsymbol{a}$ 为长度为1的方向向量。对于 $[\boldsymbol{a}]_{\times}$，有两个性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$[\boldsymbol{a}]&lt;em&gt;{\times}[\boldsymbol{a}]&lt;/em&gt;{\times} = \boldsymbol{a}\boldsymbol{a}^T - I$&lt;/li&gt;
&lt;li&gt;$[\boldsymbol{a}]&lt;em&gt;{\times}[\boldsymbol{a}]&lt;/em&gt;{\times}[\boldsymbol{a}]&lt;em&gt;{\times} = -[\boldsymbol{a}]&lt;/em&gt;{\times}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用这两个性质，指数映射可以写为:
$$
exp([\boldsymbol{w}]&lt;em&gt;{\times}) = exp(\theta [\boldsymbol{a}]&lt;/em&gt;{\times})
= \sum_{n=0}^{\infty} \frac{(\theta [\boldsymbol{a}]_{\times})^n}{n!}\&lt;br&gt;
= I + \theta [\boldsymbol{a}]_{\times} + \frac{1}{2!} \theta^2 [\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \frac{1}{3!}\theta^3[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \frac{1}{4}\theta^4([\boldsymbol{a}]_{\times})^4 + \cdots\&lt;br&gt;
= \boldsymbol{a}\boldsymbol{a}^T - [\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \frac{1}{2!}\theta^2[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \frac{1}{3!}\theta^3[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \frac{1}{4!}\theta^4([\boldsymbol{a}]_{\times})^4 + \cdots\&lt;br&gt;
= \boldsymbol{a}\boldsymbol{a}^T - [\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + \theta[\boldsymbol{a}]_{\times} + \frac{1}{2!}\theta^2[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} - \frac{1}{2!}\theta^3[\boldsymbol{a}]_{\times} - \frac{1}{4!}\theta^4([\boldsymbol{a}]_{\times})^2 + \cdots\&lt;br&gt;
= \boldsymbol{a}\boldsymbol{a}^T + (\theta - \frac{1}{3!}\theta^3 + \frac{1}{5!}\theta^5 - \cdots)[\boldsymbol{a}]_{\times} - (1 - \frac{1}{2!}\theta^2 + \frac{1}{4!}\theta^4 - \cdots)[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times}\&lt;br&gt;
=[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + I + [\boldsymbol{a}]_{\times}sin\theta - [\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} cos\theta\&lt;br&gt;
= (1 - cos\theta)[\boldsymbol{a}]_{\times}[\boldsymbol{a}]_{\times} + I + [\boldsymbol{a}]_{\times} sin\theta\&lt;br&gt;
= (1-cos\theta)(\boldsymbol{a}\boldsymbol{a}^T - I) + I + [\boldsymbol{a}]_{\times}sin\theta\&lt;br&gt;
= cos\theta\ I + (1 - cos\theta)\boldsymbol{a}\boldsymbol{a}^T + [\boldsymbol{a}]_{\times}sin\theta
\tag{12}
$$
公式 (12) 就是&lt;code&gt;罗德里格斯公式&lt;/code&gt;，&lt;u&gt;指数映射即罗德里格斯公式&lt;/u&gt;！&lt;/p&gt;
&lt;p&gt;同样，我们可以定义&lt;code&gt;对数映射&lt;/code&gt;，把 $SO(3)$ 中的元素对应到 $\mathfrak{so}(3)$ 中 :
$$
\boldsymbol{[w]}&lt;em&gt;{\times} = ln(R)^{V} = (\sum&lt;/em&gt;{n=0}^{\infty} \frac{(-1)^n}{n+1} (R - I)^{n+1} )^{V} \tag{13}
$$
不过，一般不会按照泰勒展开计算对数映射，而是通过旋转矩阵恢复李代数：
令转轴为 $\boldsymbol{n}$，转角为 $\theta$，
(1) 计算转角 $\theta$。对于转角 $\theta$，由罗德里格斯公式，有
$$
tr(R) = cos\theta\ tr(I) + (1-cos\theta)tr(\boldsymbol{n}\boldsymbol{n}^T) + sin\theta\ tr([\boldsymbol{n}]_{\times})\&lt;br&gt;
= 3cos\theta + (1-cos\theta) = 1 + 2cos\theta
$$
，因此，$\theta = arc\ cos\frac{tr(R) - 1}{2}$。
(2) 计算转轴 $\boldsymbol{n}$，由于旋转轴上的向量在旋转后不发生改变，有 $R\boldsymbol{n} = \boldsymbol{n}$。因此，转轴 $\boldsymbol{n}$ 是矩阵 $R$ 特征值为1对应的特征向量。求解次方程，再归一化，就得到了转轴。最后，李代数可以写为 $\boldsymbol{w} = \theta\boldsymbol{n}$。&lt;/p&gt;
&lt;h3 id=&#34;3-四元数-quanternion&#34;&gt;3. 四元数 (Quanternion)&lt;/h3&gt;
&lt;p&gt;四元数是Hamilton找到的一种扩展的复数，拥有一个实部和三个虚部，可表示为 :
$$
\boldsymbol{q} = (c, \boldsymbol{v}) = (q_0, q_1, q_2, q_3) = q_0 + q_1\boldsymbol{i} + q_2\boldsymbol{j} + q_3\boldsymbol{k} \tag{14}
$$
这三个虚部满足以下关系:
$$
i^2 = j^2 = k^2 = -1\&lt;br&gt;
ij = k,\ ji = -k,\&lt;br&gt;
jk = i,\ kj = -i,\&lt;br&gt;
ki = j,\ ik = -j.
$$&lt;/p&gt;
&lt;h4 id=&#34;31-四元数的运算&#34;&gt;3.1 四元数的运算&lt;/h4&gt;
&lt;p&gt;设 $\boldsymbol{q} = (c_1, \boldsymbol{v}_1), q_2 = (c_2, \boldsymbol{v}_2)$，则
$$
\boldsymbol{q}_1 \pm \boldsymbol{q}_2 = (c_1 \pm c_2,\ \boldsymbol{v}_1 \pm \boldsymbol{v}_2)\&lt;br&gt;
\boldsymbol{q}_1 \cdot \boldsymbol{q}_2 = (c_1c_2 - \boldsymbol{v}_1^T\boldsymbol{v}_2,\ c_1\boldsymbol{v}_2 + c_2\boldsymbol{v}_1 + \boldsymbol{v}_1 \times \boldsymbol{v}_2)\&lt;br&gt;
||\boldsymbol{q}|| = \sqrt{q_0^2 + q_1^2 + q_2^2 + q_3^2},\
\boldsymbol{q}^{-1} = \frac{1}{||q||^2} (c,\ -\boldsymbol{v})\&lt;br&gt;
||\boldsymbol{q}_1 \cdot \boldsymbol{q}_2|| = ||\boldsymbol{q_1}|| \cdot ||\boldsymbol{q_2}||
$$&lt;/p&gt;
&lt;h4 id=&#34;32-四元数表示旋转&#34;&gt;3.2 四元数表示旋转&lt;/h4&gt;
&lt;p&gt;假设旋转绕单位向量 $\boldsymbol{n} = (n_x, x_y, n_z)^T$ 进行了角度为 $\theta$ 的旋转，则该旋转的四元数定义为:
$$
\boldsymbol{q} = (cos\frac{\theta}{2},\ n_xsin\frac{\theta}{2},\ n_ysin\frac{\theta}{2},\ n_zsin\frac{\theta}{2})^T \tag{15}
$$
令 $\theta = \theta + 2\pi$，则 $\boldsymbol{q} = (-cos\frac{\theta}{2}, -n_x\sin\frac{\theta}{2}, -n_ysin\frac{\theta}{2}, -n_zsin\frac{\theta}{2})^T = -\boldsymbol{q}$。即 &lt;strong&gt;$\boldsymbol{q}$ 和 $-\boldsymbol{q}$ 表示同一个旋转&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;令 $\boldsymbol{v} = (n_x, n_y, n_z)$，$\boldsymbol{w} = \theta\boldsymbol{v}$，则 $R = exp([\boldsymbol{w}]_{\times})$。&lt;/p&gt;
&lt;h3 id=&#34;4-旋转矩阵角轴表示法四元数之间的转换&#34;&gt;4. 旋转矩阵、角轴表示法、四元数之间的转换&lt;/h3&gt;
&lt;h4 id=&#34;41-旋转矩阵与四元数&#34;&gt;4.1 旋转矩阵与四元数&lt;/h4&gt;
&lt;h5 id=&#34;四元数到旋转矩阵的转换&#34;&gt;四元数到旋转矩阵的转换&lt;/h5&gt;
&lt;p&gt;通过 3.2 节，即可通过四元数计算得到旋转矩阵。&lt;/p&gt;
&lt;h5 id=&#34;旋转矩阵到四元数的转换&#34;&gt;旋转矩阵到四元数的转换&lt;/h5&gt;
&lt;p&gt;令旋转矩阵为
$$
R =
\left[
\begin{array}{ccc}
R_{11} &amp;amp; R_{12} &amp;amp; R_{13}\&lt;br&gt;
R_{21} &amp;amp; R_{22} &amp;amp; R_{23}\&lt;br&gt;
R_{31} &amp;amp; R_{32} &amp;amp; R_{33}
\end{array}
\right]
$$
则旋转矩阵到四元数的转换如下:
$q_0 = \frac{\sqrt{tr(R) + 1}}{2},\ q_1 = \frac{R_{23} - R_{32}}{4q_0},\ q_2 = \frac{R_{31} - R_{13}}{4q_0},\ q_3 = \frac{R_{12} - R{21}}{4q_0}$&lt;/p&gt;
&lt;h4 id=&#34;42-四元数与角轴表示法&#34;&gt;4.2 四元数与角轴表示法&lt;/h4&gt;
&lt;h5 id=&#34;四元数到角轴表示法的转换&#34;&gt;四元数到角轴表示法的转换&lt;/h5&gt;
&lt;p&gt;采用 3.2 节的符号表示，很容易得到
$$
\theta = 2arc cos\ q_0,\&lt;br&gt;
(n_x, n_y, n_z)^T = \frac{1}{sin\frac{\theta}{2}} (q_1, q_2, q_3)^T = \frac{1}{||\boldsymbol{n}||} (q_1, q_2, q_3)^T
$$&lt;/p&gt;
&lt;h5 id=&#34;角轴表示法到四元数的转换&#34;&gt;角轴表示法到四元数的转换&lt;/h5&gt;
&lt;p&gt;通过 3.2 节即可得到。&lt;/p&gt;
&lt;h4 id=&#34;43-旋转矩阵与角轴表示法的转换&#34;&gt;4.3 旋转矩阵与角轴表示法的转换&lt;/h4&gt;
&lt;h5 id=&#34;角轴表示法到旋转矩阵&#34;&gt;角轴表示法到旋转矩阵&lt;/h5&gt;
&lt;p&gt;很显然，通过指数映射，使用罗德里格斯公式就可以将角轴表示法变换到旋转矩阵&lt;/p&gt;
&lt;h5 id=&#34;旋转矩阵到角轴表示法&#34;&gt;旋转矩阵到角轴表示法&lt;/h5&gt;
&lt;p&gt;一般会先将旋转矩阵转换到四元数，再通过四元数转换到角轴表示法。&lt;/p&gt;
&lt;h3 id=&#34;5-罗德里格斯旋转公式-rodrigues-rotation-formula&#34;&gt;5. 罗德里格斯旋转公式 (Rodrigues&amp;rsquo; rotation formula)&lt;/h3&gt;
&lt;p&gt;和罗德里格斯公式不同(李代数到李群的指数映射)，&lt;code&gt;罗德里格斯旋转公式&lt;/code&gt;是用于对向量进行旋转的，即&lt;u&gt;给定空间中的一个三维点(具体形式是一个三维向量)，如何通过轴角表示法对三维点进行旋转&lt;/u&gt;。这个公式也被扩展用于计算指数映射，也就是罗德里格斯公式。罗德里格斯旋转公式可以通过以下公式计算得到:
$$
\boldsymbol{v}_{rot} = \boldsymbol{v}cos\theta + (\boldsymbol{k}\times \boldsymbol{v})sin\theta + \boldsymbol{k}(\boldsymbol{k} \cdot \boldsymbol{v})(1-cos\theta) \tag{16}
$$&lt;/p&gt;
&lt;h4 id=&#34;51-对三维点进行旋转&#34;&gt;5.1 对三维点进行旋转&lt;/h4&gt;
&lt;p&gt;实际中，使用轴角表示法对三维点进行旋转会分两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$||\boldsymbol{w}||$ 相比0较大时。这个时候可以通过&lt;code&gt;罗德里格斯旋转公式&lt;/code&gt;计算&lt;/li&gt;
&lt;li&gt;$||\boldsymbol{w}||$ 相比0较小时。根据罗德里格斯公式，
$$
R = cos||\boldsymbol{w}||I + \frac{[\boldsymbol{w}]&lt;em&gt;{\times}}{||\boldsymbol{w}||} sin(||\boldsymbol{w}||) + \frac{[\boldsymbol{w}]&lt;/em&gt;{\times}^2}{||\boldsymbol{w}||^2}(1 - cos(||\boldsymbol{w}||))
$$
当$\boldsymbol{w}$较小时，$cos(||\boldsymbol{w}||) \approx 1, sin(||\boldsymbol{w}||) \approx ||\boldsymbol{w}||$。因此，$R \approx I + [\boldsymbol{w}]&lt;em&gt;{\times}$。于是，$R\boldsymbol{p} = \boldsymbol{p} + [\boldsymbol{w}]&lt;/em&gt;{\times} \boldsymbol{p}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;注： 对于李代数 $\boldsymbol{w} = \theta \boldsymbol{n}$，其中$\boldsymbol{n}$为单位转轴，因此 $||\boldsymbol{n}||^2 = 1$，于是$||\boldsymbol{w}||^2 = \theta^2 ||\boldsymbol{n}||^2 = \theta^2$。因此，有 $\theta = ||\boldsymbol{w}||$&lt;/em&gt;。&lt;/p&gt;
&lt;h4 id=&#34;52-ceres中的实现&#34;&gt;5.2 Ceres中的实现&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename T&amp;gt; inline
void AngleAxisRotatePoint(const T angle_axis[3], const T pt[3], T result[3]) {
  const T theta2 = DotProduct(angle_axis, angle_axis);
  if (theta2 &amp;gt; T(std::numeric_limits&amp;lt;double&amp;gt;::epsilon())) {
    // Away from zero, use the rodriguez formula
    //
    //   result = pt costheta +
    //            (w x pt) * sintheta +
    //            w (w . pt) (1 - costheta)
    //
    // We want to be careful to only evaluate the square root if the
    // norm of the angle_axis vector is greater than zero. Otherwise
    // we get a division by zero.
    //
    const T theta = sqrt(theta2);
    const T costheta = cos(theta);
    const T sintheta = sin(theta);
    const T theta_inverse = T(1.0) / theta;

    const T w[3] = { angle_axis[0] * theta_inverse,
                     angle_axis[1] * theta_inverse,
                     angle_axis[2] * theta_inverse };

    // Explicitly inlined evaluation of the cross product for
    // performance reasons.
    const T w_cross_pt[3] = { w[1] * pt[2] - w[2] * pt[1],
                              w[2] * pt[0] - w[0] * pt[2],
                              w[0] * pt[1] - w[1] * pt[0] };
    const T tmp =
        (w[0] * pt[0] + w[1] * pt[1] + w[2] * pt[2]) * (T(1.0) - costheta);

    result[0] = pt[0] * costheta + w_cross_pt[0] * sintheta + w[0] * tmp;
    result[1] = pt[1] * costheta + w_cross_pt[1] * sintheta + w[1] * tmp;
    result[2] = pt[2] * costheta + w_cross_pt[2] * sintheta + w[2] * tmp;
  } else {
    // Near zero, the first order Taylor approximation of the rotation
    // matrix R corresponding to a vector w and angle theta is
    //
    //   R = I + hat(w) * sin(theta)
    //
    // But sintheta ~ theta and theta * w = angle_axis, which gives us
    //
    //  R = I + hat(w)
    //
    // and actually performing multiplication with the point pt, gives us
    // R * pt = pt + w x pt.
    //
    // Switching to the Taylor expansion near zero provides meaningful
    // derivatives when evaluated using Jets.
    //
    // Explicitly inlined evaluation of the cross product for
    // performance reasons.
    const T w_cross_pt[3] = { angle_axis[1] * pt[2] - angle_axis[2] * pt[1],
                              angle_axis[2] * pt[0] - angle_axis[0] * pt[2],
                              angle_axis[0] * pt[1] - angle_axis[1] * pt[0] };

    result[0] = pt[0] + w_cross_pt[0];
    result[1] = pt[1] + w_cross_pt[1];
    result[2] = pt[2] + w_cross_pt[2];
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;6-so3-上的距离测量-distance-measure&#34;&gt;6. $SO(3)$ 上的距离测量 (Distance Measure)&lt;/h3&gt;
&lt;h5 id=&#34;1-bi-invariant-distance&#34;&gt;(1) Bi-invariant distance&lt;/h5&gt;
&lt;p&gt;如果对于所有的 $S$ 和 $R_i$，我们有 :
$$
d(SR_1, SR_2) = d(R_1, R_2) = d(R_1S, R_2S) \tag{17}
$$
那么这样的一个距离测量称为 &lt;code&gt;Bi-invariant&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&#34;2-angular-distance-geodesic-distance&#34;&gt;(2) Angular Distance (Geodesic Distance)&lt;/h5&gt;
&lt;p&gt;定义 R 和 S 之间的 &lt;code&gt;angular distance&lt;/code&gt; 为旋转 $SR^T$ 的角度，并且处于区间 $[0, \pi]$ 里。因此，
$$
d(S, R) = d(SR^T, I) = ||log(SR^T)||_2 \tag{18}
$$
距离测量函数 $d(S, R)$ 和 $SR^T$ 的旋转角度相等。需要注意，我们可能会等价地写为 $R^TS$，$RS^T$，$S^TR$，因为这些都表示同一个旋转。&lt;/p&gt;
&lt;h5 id=&#34;3-chordal-distance&#34;&gt;(3) Chordal Distance&lt;/h5&gt;
&lt;p&gt;旋转 $R$ 和 $S$ 之间的 &lt;code&gt;chordal distance&lt;/code&gt; 定义为:
$$
d_{chord}(S, R) = ||S-R||_F \tag{19}
$$
其中，$||\cdot||_F$ 表示矩阵的 &lt;code&gt;Frobenius 范数&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;chordal distance&lt;/code&gt; 和 &lt;code&gt;angular distance&lt;/code&gt; 之间可以通过罗德里格斯公式关联起来:
$$
exp(\theta \boldsymbol{v}) = I + sin\theta\ [\boldsymbol{v}]&lt;em&gt;{\times} + (1-cos\theta)([\boldsymbol{v}]&lt;/em&gt;{\times})^2
$$
具体来讲，令 $SR^T = exp(\theta \boldsymbol{v})$，由于 $[\boldsymbol{v}]&lt;em&gt;{\times}$ 和 $([\boldsymbol{v}]&lt;/em&gt;{\times})^2$正交，且 $||[\boldsymbol{v}]&lt;em&gt;{\times}||^2_F = ||([\boldsymbol{v}]&lt;/em&gt;{\times})^2||^2_F = 2$，因此，我们有
$$
d_{chord}(S, R)^2 = ||S - R||^2 = ||SR^T - I||^2 = 2(sin^2\theta + (1-cos\theta)^2) = 8sin^2\frac{\theta}{2}
$$
因此，
$$
d_{chord}(S, R) = 2\sqrt{2} sin\frac{\theta}{2} \tag{20}
$$&lt;/p&gt;
&lt;h3 id=&#34;7-扰动模型对旋转求导&#34;&gt;7. 扰动模型对旋转求导&lt;/h3&gt;
&lt;p&gt;设左扰动 $\nabla R$ 对应的李代数为 $\boldsymbol{a}$。对 $\boldsymbol{a}$ 求导，有
$$
\frac{\delta(R\boldsymbol{p})}{\delta \boldsymbol{a}} = {\lim_{\boldsymbol{a} \to 0}} \frac{exp([\boldsymbol{a}]_{\times}) exp([\boldsymbol{a}]_{\times})\boldsymbol{p} - exp([\boldsymbol{a}]_{\times})\boldsymbol{p}}{\boldsymbol{a}} \&lt;br&gt;
= {\lim_{\boldsymbol{a} \to 0}} \frac{(1 + [\boldsymbol{a}]_{\times}) exp([\boldsymbol{a}]_{\times})\boldsymbol{p} - exp([\boldsymbol{a}]_{\times})\boldsymbol{p}}{\boldsymbol{a}}\&lt;br&gt;
= {\lim_{\boldsymbol{a} \to 0}} \frac{[\boldsymbol{a}]_{\times}R\boldsymbol{p}}{\boldsymbol{a}} = {\lim_{\boldsymbol{a} \to 0}} \frac{-[R\boldsymbol{p}]_{\times}\boldsymbol{a}}{\boldsymbol{a}} = -[R\boldsymbol{p}]_{\times}
$$&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[1] 视觉SLAM十四讲 从理论到实践&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula&#34;&gt;https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] &lt;a href=&#34;https://en.wikipedia.org/wiki/Axis%E2%80%93angle_representation&#34;&gt;https://en.wikipedia.org/wiki/Axis%E2%80%93angle_representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] Hartley, Richard, Trumpf, et al. Rotation Averaging[J]. International Journal of Computer Vision, 2013, 103(3):267-305.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>View Graph</title>
      <link>https://AIBluefisher.github.io/post/view-graph/</link>
      <pubDate>Thu, 04 Apr 2019 11:02:09 +0800</pubDate>
      <guid>https://AIBluefisher.github.io/post/view-graph/</guid>
      <description>&lt;p&gt;首先我们来看看什么是view graph以及为什么view graph在重建中有什么作用？&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A view graph is a graph which nodes represent images and edges represent the number of matches between two nodes.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;view graph&lt;/strong&gt; 实际上就是一个图，节点对应图片，边对应两幅图片之间的一些连接关系(可能包括匹配的数量，也包括两幅图片之间的一个相对运动关系)。view graph也叫做 &lt;strong&gt;Match Graph&lt;/strong&gt;, 首先在[2]中提出一个比较明确的定义。&lt;/p&gt;
&lt;p&gt;我们直到，特征点匹配一直是三维重建中一个瓶颈-不仅仅在于匹配比较费时间，而且很多错误的匹配也能通过geometric verification, 从而影响重建精度。而对于单对图片的匹配来说，提出新的加速算法也不能有效的提升匹配效率。从下表可以看到，最好的算法每对图片匹配也得花0.1s(这里我们考虑的是SIFT的128维描述子，而不是ORB之类的二进制描述子，并且ORB效果上不如SIFT)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/match_time.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设有1000张图片，那么穷举所有的情况需要做1000*(1000-1)/2次匹配。如果每次匹配花费0.1s，那么大约总共需要花费14个小时来做完这些匹配。而且这还只是1000张图片的情况！那么对于大规模的数据集(10000, 100000, &amp;hellip;)，在不考虑硬件方面提升的情况下，该怎么来加速匹配呢？&lt;/p&gt;
&lt;p&gt;这就是view graph要做的事 - 通过减少大量不必要的匹配(很有可能是错配)，来提升匹配速度。&lt;/p&gt;
&lt;h2 id=&#34;1-vocabulary-tree&#34;&gt;1. Vocabulary Tree&lt;/h2&gt;
&lt;p&gt;词汇树最初是用来做object recognition的，但是在三维重建中(SfM和SLAM)也被大量使用。&lt;/p&gt;
&lt;h3 id=&#34;11-词汇树的构建&#34;&gt;1.1 词汇树的构建&lt;/h3&gt;
&lt;p&gt;词汇树实际上就是一棵多层k叉树，通过反复的k-means聚类来得到。假设我们已经得到特征点的描述子，这些描述子则被用来构建词汇树。和原来的k-means聚类不同的是，原来的k指的是最终的聚类数量，而词汇树中的k指的是每一次迭代中的分支数目。首先，一个初始的k-means用于定义k个聚类中心，然后这些数据就被分成k组，每个组所包含的描述子到k-means聚类中心的距离最近。然后，该过程被反复调用，直到达到一个人为设定的参数-L层。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/voc_tree_generation.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;词汇树的内存大小和叶节点的数量成正比。对于 $D$ 维的描述子，构建一棵 $L$ 层 $k$ 个分支的词汇树，需要$Dk^L$ 字节(假设描述子的每个维度占用1字节)。因此，对于128维的SIFT描述子，构建一棵6层的10分支的词汇树，会产生1M的叶节点，需要占用143M的内存。&lt;/p&gt;
&lt;h3 id=&#34;12-评分score的定义&#34;&gt;1.2 评分(score)的定义&lt;/h3&gt;
&lt;p&gt;其实之前一直不太明白词汇树的评分怎么能够定义图片之间的相似度。然后看了参考文献[1]。词汇树的相似度借用了文本检索的方法。首先来看看文本检索中相似度怎么算的。&lt;/p&gt;
&lt;p&gt;假设一个拥有k个单词的词汇表，那么每个文本可以被一个 $k$ 维向量表示：
$$V_d=(t_1,&amp;hellip;,t_i,&amp;hellip;,t_k)^T$$
其中，每一项的计算公式为 $$t_i=\frac{n_{id}}{n_d}log\frac{N}{n_i}$$
其中，$n_{id}$ 是单词 $i$ 在文档 $d$ 中出现的次数，$n_d$ 是文档d中的单词总数，$n_i$ 是单词i在整个数据库中出现的次数，N是整个数据库中的文档数。
这个权重定义方法称为Term Frequency-Inver Document frequency (TF-IDF) - 它由两项的乘积组成：word frequency $n_{id}/n_d$ 以及 inverse document frequency $logN/n_i$。这么做的intuition是(直接翻译不太好讲，就借用原文)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;word frequency weights words occurring ofter in a particular document, and thus describe it well, whilst the inverse document frequency down-weights words that appear ofter in the database.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在检索的时候，文本根据查询向量v_q和其他所有向量v_d之间的归一化标量积(normalized scalar product)来排名。&lt;/p&gt;
&lt;p&gt;但是在[2]里，对于图片之间的相似度评分，稍微做了修改。在词汇树中，查询图片和数据库中图片的相关性需要通过比较数据库中描述子沿着词汇树的路径和查询图片描述子沿着词汇树的路径的相似性(大致理解就是这样，原文是: we wish to determine the relevance of a database image to the query image based on how similar the paths down the vocabulary tree are for the descriptors from the database image and the query image)。[2]中的方法是：对词汇树中的每一个节点都赋予一个权值$n_{id}/w_i$，并且定义查询向量 $q_i$ 和数据库向量$d_i$：&lt;/p&gt;
&lt;p&gt;$$
q_i=n_iw_i,\ d_i=m_iw_i
$$&lt;/p&gt;
&lt;p&gt;其中 $n_i$ 和 $m_i$ 分别是所查询图片和数据库图片的描述子向量通过节点i的数量。$w_i$ 的定义和[1]中IDF的定义相似：$$log\frac{N}{N_i}$$&lt;/p&gt;
&lt;p&gt;其中N是数据库中图片的数量，$N_i$ 是数据库中至少有一个描述子向量通过节点i的图片数量。然后查询向量和数据库向量之间的相似度评分通过下面的公式来定义：&lt;/p&gt;
&lt;p&gt;$$s(q, d)=\lVert \frac{q}{\lVert q \rVert } - \frac{d}{\lVert d \rVert } \rVert$$&lt;/p&gt;
&lt;p&gt;实际上就是两个归一化之后的向量的差的范数。上面公式中的范数理论上可以使用任意形式的范数，但是[2]中发现L1-范数比L2-范数得到的结果要更好。&lt;/p&gt;
&lt;h3 id=&#34;13-评分的实现&#34;&gt;1.3 评分的实现&lt;/h3&gt;
&lt;p&gt;为了高效地进行评分，[2]使用了 inverted file 。inverted file存储的是某个节点出现在的图片的id(这个节点有可能在很多张图片中出现，因此存储了多个id)，以及该描述子在数据库中出现的总数。可以通过下图理解:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/inverted_file.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;[2]中的实现只存储了叶节点的inverted file,而对于内部节点的inverted file，就是叶节点的inverted files的简单的连接。也可以通过下图理解：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/inverted_file_store.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;到此，词汇树的基本上讲解完了，更细节的内容还是见[1,2]两篇参考文献。对于vocabulary tree的实现，可以见Tianwei的工作
&lt;a href=&#34;https://github.com/hlzz/libvot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;libvot&lt;/a&gt;以及Noah Snavely大神的实现
&lt;a href=&#34;https://github.com/snavely/VocabTree2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VocabTree2&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;2-build-rome-in-a-day-briadsup3sup&#34;&gt;2. Build Rome in a Day (BRIAD)&lt;sup&gt;[3]&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;在[3]中，词汇树用于计算图片之间的相似度。对于每张图片，取相似度得分前k1+k2张图片，其中的k1张图片用于初始的建图步骤，额外的k2张图片用于对连通分量(connected components)进行扩展/增大。在初始图构建完之后，可能存在多个连通分量。也就是说，整个场景肯定不是完整的。对于连通分量大于等于2的情况，接下来的k2张图片便被用于连接起这些连通分量。需要注意的是，只有处于不同连通分量之间的图片对才被考虑进来。这个过程称作&amp;quot;Components merging&amp;rdquo;。在component merging之后，匹配图通常还不够稠密来较为可靠地进行重建。因此，“Query Expansion&amp;quot;的步骤被提出来强化view graph. Query expansion的意思就是，如果图片i和图片j是一个正确匹配(匹配点的个数足够并且通过了geometric verification)，图片j和图片k是一个正确的匹配，那么接下来就尝试验证i和k是不是一个正确的匹配。Query expansion会在迭代一个固定的次数之后终止，也就得到了最终的一个match graph。&lt;/p&gt;
&lt;p&gt;但是，query expansion存在一个缺点：当query expansion重复一定次数之后，用于进行匹配的图片很快就和原图片相差很大了(也就意味着相似度不够，可能是错误的匹配)。正因为做了这些错误的匹配，query expansion会导致drift。尽管geometric verification用于筛除不满足对极几何的匹配，但是这些不必要的校验依然会降低匹配效率。&lt;/p&gt;
&lt;h2 id=&#34;3-graph-based-consistency-matching&#34;&gt;3. Graph-based Consistency Matching&lt;/h2&gt;
&lt;p&gt;这篇工作尝试通过回环一致性来检测坏的匹配对。它的intuition在于：错误的匹配会造成较大的误差累积，并且一些错误的匹配能够在更长的路径中通过回环一致性检测出来。&lt;/p&gt;
&lt;p&gt;为了检验回环一致性，这篇文章中定义了两个术语：Weak Consistency和Strong Consistency。Weak consistency对应于较短路径下的回环一致性(三个节点两条边，路径长度为2)，Strong consistency对应于较长路径下的回环一致性 (路径长度不是一个定值)。consistency指的是图片间的相对运动通过矩阵链乘得到的结果和单位阵之差的范数应当小于某个阈值。&lt;/p&gt;
&lt;p&gt;为了找到一个一致的match graph，需要对以下三个性能标准进行平衡:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完整性 (completeness)&lt;/li&gt;
&lt;li&gt;效率 (efficiency)&lt;/li&gt;
&lt;li&gt;一致性 (consistency)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过这三个标准，Shen [4]的方法可以找到一个能够进行完整重建的匹配对的下界。该算法包含下面三个步骤。&lt;/p&gt;
&lt;h3 id=&#34;31-match-graph-initilization&#34;&gt;3.1 Match Graph Initilization&lt;/h3&gt;
&lt;p&gt;Shen [4]使用词汇树得到的相似度评分作为一个初始图的权值 (实际上该权值的定义没有这么简单，这里为了简便就这样写，具体可以见参考文献)，然后构建出一棵最小生成树。需要注意的是，如果图片集合包含一些孤立的视图(singleton views)或者分块的场景，图的初始化过程可能会耗费比较长的时间，因为我们需要遍历每一条边来尝试把singleton image加入到图里面。这个图初始化算法比较简单，可以通过对Kruskal算法做略微的修改得到。&lt;/p&gt;
&lt;h2 id=&#34;32-graph-expansion-by-strong-triplets&#34;&gt;3.2 Graph Expansion by Strong Triplets&lt;/h2&gt;
&lt;p&gt;这个过程称作三元组扩展。其实思路和[3]的query expansion差不多： 如果图片i和图片j是一个正确匹配，图片j和图片k是一个正确的匹配，那么接下来就尝试验证i和k是不是一个正确的匹配。不过稍有不同的是，这个过程还验证了loop consistency。而且，为了保证效率，triplets expansion的过程只做三次(因为做所有的triplets expansion的时间复杂度是O(n^3))。整个过程如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/ge.png&#34; alt=&#34;ge&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-component-merging&#34;&gt;3.3 Component Merging&lt;/h3&gt;
&lt;p&gt;这一步是对triplets expansion步骤的一个扩展。在triplets expansion中，只有局部的几何信息被考虑到，这意味着并没有考虑一个全局的回环结构，因此也就无法反映出整个数据集的潜在的位姿图。除此之外，更长的回环被忽略掉了。因此，component merging这一步被提出来强化之前的match graph。&lt;/p&gt;
&lt;p&gt;通过community detection，图被划分成几块区域，区域内的图片有着较为紧密的联系，区域之间的图片联系较弱。因此，组间的更长的路径被用于检验回环一致性。假设图片i在编号为k1的组内，图片j在编号为k2的组内，接着，使用广度优先搜索算法寻找这两张图片的最短路径(这个时候，边的权值设为1)，然后所有边之间的相对运动的矩阵表示便被用于检验strong consistency.&lt;/p&gt;
&lt;p&gt;需要注意的是，一般情况下，图割算法可以作为community detection的一个代替。&lt;/p&gt;
&lt;h2 id=&#34;4-graphmatch&#34;&gt;4. GraphMatch&lt;/h2&gt;
&lt;p&gt;尽管词汇树能够用于快速寻找潜在的匹配对，但是，词汇树仍然存在一些缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;构建一棵词汇树的代价比较大(当然，现在有一些训练好的词汇树可以用)并且需要较大的内存。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在大规模的图片中，SfM系统需要将所有图片的索引加载到内存中来寻找合适的图片对进行匹配。这种情况下，潜在的内存开系哦啊依然非常大&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，词汇树并不是一种很可信的寻找正确匹配的方法 - 用词汇树来对图片进行表示对于匹配来说提供的信息非常有限，就像下图所示，词汇树的相似度评分对于匹配和非匹配的概率分布图来说，重合度非常大(也就是说，词汇树并不能有效区分匹配和不匹配的图片)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/vt.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;因此， GraphMatch[4]通过实验，对比使用了一些比词汇树更能有效地区分匹配和不配的图片对的方法。他们的实验结果表明，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Fisher vector distances are faster to compute and more indicative of a possible match than vocabulary tree similarity scores.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如下面的概率分布图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://AIBluefisher.github.io/img/view_graph/compare.png&#34; alt=&#34;vt&#34;&gt;&lt;/p&gt;
&lt;p&gt;GraphMatch算法分为三个步骤(实际上和之前的算法大同小异)：&lt;/p&gt;
&lt;h3 id=&#34;41-预处理-pre-processing&#34;&gt;4.1 预处理 (Pre-processing)&lt;/h3&gt;
&lt;p&gt;首先，计算SIFT特征以及描述子，然后，使用Fisher-vector encoder来计算Fisher vector。在每一张图片的Fisher vector都计算完之后，GraphMatch对每对图片都计算一个distance matrix。最后，这些Fisher distances用来对所有图片进行一个相似度的排名。越小的Fisher distance意味着越高的相似度。&lt;/p&gt;
&lt;h3 id=&#34;42-采样和冒泡-sampling-and-propagation&#34;&gt;4.2 采样和冒泡 (Sampling and Propagation)&lt;/h3&gt;
&lt;p&gt;实际上，这里的采样和冒泡个人觉得仅仅是这篇文章包装之后的说法，和前面两篇论文的做法并无本质上的区别。&lt;/p&gt;
&lt;p&gt;采样是指，对于之前排名的结果，每张图片都选一个固定的值来选取图片进行匹配。而冒泡指的是(还是引用原文吧，翻译感觉没必要):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Given a pair *A* and *B* that share an edge, the propagation step takes the top ranked neighbors of B and tests them against *A* and vice-versa. And GraphMatch propagates only from vertices that have less that *MAXNUMNEIGHBORS* for termination criterion.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;5-实现&#34;&gt;5. 实现&lt;/h2&gt;
&lt;p&gt;自己目前还没找到关于view graph的一个开源实现。因为view graph对于构建大规模重建系统确实是必要的，因此自己也正在实现。代码可见 
&lt;a href=&#34;&#34;&gt;XFeatures&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Sivic J, Zisserman A. Video Google: A text retrieval approach to object matching in videos[C]//null. IEEE, 2003: 1470.&lt;/p&gt;
&lt;p&gt;[3] Nister D, Stewenius H. Scalable Recognition with a Vocabulary Tree[J]. Proc Cvpr, 2006, 2(10):2161-2168.&lt;/p&gt;
&lt;p&gt;[3] Agarwal S , Snavely N , Simon I , et al. Building Rome in a day[C]// IEEE International Conference on Computer Vision. IEEE, 2009.&lt;/p&gt;
&lt;p&gt;[4] Shen T, Zhu S, Fang T, et al. Graph-Based Consistent Matching for Structure-from-Motion[C]// European Conference on Computer Vision. 2016.&lt;/p&gt;
&lt;p&gt;[5] Cui Q, Fragoso V, Sweeney C, et al. GraphMatch: Efficient Large-Scale Graph Construction for Structure from Motion[J]. 2017.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LIFT</title>
      <link>https://AIBluefisher.github.io/post/lift/</link>
      <pubDate>Thu, 26 Apr 2018 17:39:18 +0800</pubDate>
      <guid>https://AIBluefisher.github.io/post/lift/</guid>
      <description>&lt;p&gt;LIFT[1]是一种使用CNN来提取特征点(feature points / interests)的方法, 发表在ECCV 2016.&lt;/p&gt;
&lt;p&gt;众所周知, 尽管在2004年就已经发表出来, 但是SIFT(Scale Invariant Feature Transform)在三维重建中一直至今都无可替代. 随着学习的火热, 研究人员开始将目光更多转向于寻求一种基于学习的方法来提取特征点, 从而代替人工设计的特征点.&lt;/p&gt;
&lt;h2 id=&#34;1motivation&#34;&gt;1.Motivation&lt;/h2&gt;
&lt;p&gt;SIFT提取特征点包括三个步骤: 关键点检测器检测候选特征点、方向赋值、计算128维描述子向量. 在LIFT之前的工作中, 有很多使用CNN来学习这三个步骤的算法, 然而LIFT是第一个提出将这三个网络整合到一起并提出成功的训练方法.&lt;/p&gt;
&lt;p&gt;LIFT的作者发现, 从零开始训练整个网络是不可能的, 因为这三个神经网络的目的不同. 因此, 他们提出分别训练这三个网络来解决这个问题: 首先训练描述子(Descriptor), 然后是方向估计子(Orientation Estimator), 最后是特征点检测器(detector).&lt;/p&gt;
&lt;h2 id=&#34;2训练数据&#34;&gt;2.训练数据&lt;/h2&gt;
&lt;p&gt;LIFT的训练数据来源于使用Structure from Motion(SfM)算法得到的特征点, 而SfM算法的输入为在不同视点(viewpoints)和光照条件下获取到的图像.&lt;/p&gt;
&lt;p&gt;实际上, LIFT使用的训练数据并非一个个像素点, 而是一些图像块(image patches). 为了使得优化过程更易于处理, 这些图像块在不同的尺度空间中提取得到(这句话没太理解, 先留作疑问. 原文是: &amp;ldquo;We formulate this training problem on image patches extracted at different scales to make the optimization tractable&amp;rdquo;).&lt;/p&gt;
&lt;h2 id=&#34;3方法&#34;&gt;3.方法&lt;/h2&gt;
&lt;h3 id=&#34;31-训练数据的构建&#34;&gt;3.1 训练数据的构建&lt;/h3&gt;
&lt;p&gt;使用Piccadilly和Roman Forum和Visual SFM来重建三维场景. 其中Piccadilly数据集包含3384张图片, 最后的重建结果包含59k个特征点(平均每张图片6.5个特征点); Roman-Forum数据集包含1658张图片, 最后的重建结果包含51k个特征点(平均每张图片包含5.2个特征点).&lt;/p&gt;
&lt;p&gt;作者将这些数据分成训练集和验证集. 为了构建一个正的训练样本, 他们只使用了在SfM重建之后保留下的特征点(不是使用最初SIFT提取出来的特征点).
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/7.png&#34; alt=&#34;feature&#34;&gt;&lt;/p&gt;
&lt;p&gt;运行时的pipeline
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/1.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-三个网络&#34;&gt;3.2 三个网络&lt;/h3&gt;
&lt;p&gt;这部分内容还没看太懂, 之后再更~&lt;/p&gt;
&lt;h2 id=&#34;4实验结果及对比&#34;&gt;4.实验结果及对比&lt;/h2&gt;
&lt;p&gt;LIFT的作者使用了三个数据集进行对比实验:&lt;/p&gt;
&lt;p&gt;Strecha数据集. 包含从不同视角拍摄的两个场景的19张图片.
DTU数据集. 包含在不同视点和光照条件下拍摄的60张图片. 这个数据集用于检测LIFT在视点变化下的性能.
Webcam数据集. 包含同一视点拍摄的强光照变化下的6个场景的710张图片. 这个数据集用于检测LIFT在自然光照变化下的性能.
.对于Strcha和DTU数据集, LIFT的作者使用所提供的ground truth来进行匹配. 同时, 他们使用三个度量标准来评估性能:&lt;/p&gt;
&lt;p&gt;可重复性(repeatability, Rep): 这个标准用于检测feature detector的性能.
最近邻mAP(Nearest Neighbor mean Average Precision, NN mAP)
匹配得分(Matching Score, M. Score): 匹配点的ground truth可以被恢复出来的比例. 这个标准用于评估整个算法的性能.&lt;/p&gt;
&lt;h3 id=&#34;41-匹配结果对比&#34;&gt;4.1 匹配结果对比&lt;/h3&gt;
&lt;p&gt;下图展示了分别使用SIFT(左边)和LIFT(右边)在Piccadilly数据集训练之后, 在500个特征点的图像上的匹配结果. 实验结果表明LIFT能得到更多正确的匹配.
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/2.png&#34; alt=&#34;compare&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;42-整个lift网络评估&#34;&gt;4.2 整个LIFT网络评估&lt;/h3&gt;
&lt;p&gt;对于整个网络的性能评估, 论文中和多种传统方法以及基于学习的方法进行了对比.&lt;/p&gt;
&lt;p&gt;下图给出了在三个数据集中各个方法得到的average matching score.
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/3.png&#34; alt=&#34;matching score&#34;&gt;&lt;/p&gt;
&lt;p&gt;下表给出了各个方法得到的average matching score的确切数字.
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/4.png&#34; alt=&#34;matching score number&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中LIFT-pic是使用Piccadilly数据集训练, LIFT-rf使用Roman-Forum训练. 结果显示在这三个数据集上, LIFT都超过了其他方法. 其中一个有趣的结果是: 在DTU数据集上, SIFT仍然是除LIFT之外的性能最好的方法(包括使用其他基于学习方法的特征点).&lt;/p&gt;
&lt;p&gt;最后, 为了了解LIFT中, detector, orientation estimator, descriptor对于整个LIFT的影响, 作者还将LIFT中每个部分得到的结果和SIFT交换, 并比较了它们的性能, 结果如下表.
&lt;img src=&#34;https://AIBluefisher.github.io/img/lift/5.png&#34; alt=&#34;performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;结果表明, LIFT中每一个部分得到的结果都超过了SIFT的每一个部分, 并在LIFT的整个过程中都起到了重要作用.&lt;/p&gt;
&lt;h2 id=&#34;5缺点不足&#34;&gt;5.缺点/不足&lt;/h2&gt;
&lt;p&gt;尽管LIFT已经很优秀了, 但是它还存在一些明显的不足: LIFT使用Spatial Transformer和softargmax将之前工作中三个独立的网络整合到一个统一的网络中, 并且能够使用反向传播算法进行端到端的训练(感觉地位和faster-RCNN相似), 但是这也使得整个网络从零开始学习尽管在理论上可行, 实际上却是不行的. 因此, 还需要一个更有效的策略来对LIFT进行训练.&lt;/p&gt;
&lt;p&gt;除此之外, 由于只使用Internet数据集进行训练, LIFT还表现出对数据的过拟合[2], 因此下一个目标应该是使用更多数据集进行训练.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;
&lt;p&gt;寻找一种更有效的训练策略来对整个网络进行训练
使用hard negative mining策略来使用整张图片而不只是图片块对LIFT进行训练&lt;/p&gt;
&lt;h2 id=&#34;6-补充材料&#34;&gt;6. 补充材料&lt;/h2&gt;
&lt;p&gt;看视频应该更能直观地感受LIFT的效果:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hhxAttChmCo&#34;&gt;https://www.youtube.com/watch?v=hhxAttChmCo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以及, 关于LIFT的所有材料都能在这个网站上找到:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cvlab.epfl.ch/research/detect/lift&#34;&gt;https://cvlab.epfl.ch/research/detect/lift&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LIFT也在Github上公布了模型及其tensorflow实现:&lt;/p&gt;
&lt;p&gt;模型 - &lt;a href=&#34;https://github.com/cvlab-epfl/LIFT&#34;&gt;https://github.com/cvlab-epfl/LIFT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;tensorflow实现 - &lt;a href=&#34;https://github.com/cvlab-epfl/tf-lift&#34;&gt;https://github.com/cvlab-epfl/tf-lift&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;[1] Yi K M, Trulls E, Lepetit V, et al. LIFT: Learned Invariant Feature Transform[C]// European Conference on Computer Vision. Springer, Cham, 2016:467-483.&lt;/p&gt;
&lt;p&gt;[2] Schonberger J L, Hardmeier H, Sattler T, et al. Comparative Evaluation of Hand-Crafted and Learned Local Features[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2017:6959-6968.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
